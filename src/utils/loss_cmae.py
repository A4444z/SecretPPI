"""
æŸå¤±å‡½æ•°æ¨¡å— (CMAE æ¶æ„)ã€‚
å®ç° InfoNCE æŸå¤±å’ŒåŸºäºæ©ç çš„ D-RMSD é‡æ„æŸå¤±ï¼Œç”¨äºå­¦ä¹ è›‹ç™½è´¨äº’ä½œæµå½¢ã€‚
"""

import torch
import torch.nn as nn
from torch_geometric.utils import to_dense_batch
import torch.nn.functional as F
import torch.distributed as dist

# ================= 1. æ ¸å¿ƒé‡æ„æŸå¤±ï¼šMasked D-RMSD =================

class MaskedDRMSDLoss(nn.Module):
    """
    æ©ç  D-RMSD æŸå¤±å‡½æ•° (å¸¦å±€éƒ¨æˆªæ–­)ã€‚
    ğŸš¨ å…³é”®ç‰¹æ€§ï¼šåªè®¡ç®—è¢«æ©ç ï¼ˆè¢«ç ´åï¼‰çš„åŸå­ä¸å…¶å®ƒæœ‰æ•ˆåŸå­ä¹‹é—´çš„ç›¸å¯¹è·ç¦»è¯¯å·®ã€‚
    """
    
    def __init__(self, reduction='mean', cutoff=15.0):
        super().__init__()
        self.reduction = reduction
        self.cutoff = cutoff 
        
    def forward(
        self,
        pos_pred,
        pos_true,
        mask_v1,     # ğŸ‘ˆ å¿…é¡»ä¼ å…¥ forward ä¸­ç”Ÿæˆçš„ mask_v1
        batch_idx=None
    ):
        if batch_idx is None:
            batch_idx = torch.zeros(pos_pred.size(0), dtype=torch.long, device=pos_pred.device)
        
        # è½¬ä¸ºå¯†é›†çŸ©é˜µ [B, N_max, 3]
        pos_pred_dense, batch_mask = to_dense_batch(pos_pred, batch_idx)
        pos_true_dense, _ = to_dense_batch(pos_true, batch_idx)
        mask_v1_dense, _ = to_dense_batch(mask_v1.float(), batch_idx) # [B, N_max]
        
        # è®¡ç®—æˆå¯¹è·ç¦» [B, N_max, N_max]
        D_pred = torch.cdist(pos_pred_dense, pos_pred_dense, p=2.0)
        D_true = torch.cdist(pos_true_dense, pos_true_dense, p=2.0)
        
        mse = (D_pred - D_true) ** 2
        
        # 1. æœ‰æ•ˆèŠ‚ç‚¹æ©ç  (å»é™¤ padding)
        valid_2d = batch_mask.unsqueeze(1) * batch_mask.unsqueeze(2)
        
        # 2. å±€éƒ¨è·ç¦»æˆªæ–­æ©ç  (åªå…³æ³¨ cutoff å†…çš„ç‰©ç†äº’ä½œ)
        cutoff_mask = (D_true < self.cutoff).float()
        
        # å»é™¤å¯¹è§’çº¿ (è‡ªå·±åˆ°è‡ªå·±)
        eye_mask = 1.0 - torch.eye(D_true.size(1), device=D_true.device).unsqueeze(0)
        
        # 3. ğŸš¨ æ ¸å¿ƒæ©ç é€»è¾‘ï¼šè‡³å°‘æœ‰ä¸€ä¸ªåŸå­æ˜¯è¢«ç‚¸æ‰çš„ (mask_v1)
        # å¦‚æœ i å’Œ j éƒ½æ²¡è¢«ç‚¸æ‰ï¼Œé‚£æ¨¡å‹å°±æ˜¯ä½œå¼Šç…§æŠ„ï¼Œæˆ‘ä»¬ä¸å¥–åŠ±å®ƒã€‚
        # åªæœ‰å½“ i æˆ– j æ˜¯è¢«ç ´åçš„åŸå­æ—¶ï¼Œé‡æ„å…¶è·ç¦»æ‰æœ‰æ„ä¹‰ã€‚
        mask_i = mask_v1_dense.unsqueeze(2) # [B, N_max, 1]
        mask_j = mask_v1_dense.unsqueeze(1) # [B, 1, N_max]
        
        # mask_i + mask_j > 0 è¡¨ç¤ºï¼šè¿™å¯¹åŸå­ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå±äºè¢«ç ´åçš„åŒºåŸŸ
        masked_region_mask = (mask_i + mask_j > 0.5).float()
        
        # ç»„åˆæœ€ç»ˆæ©ç 
        final_mask = valid_2d * cutoff_mask * eye_mask * masked_region_mask
        
        mse = mse * final_mask
        
        if self.reduction == 'mean':
            # åˆ†æ¯ä½¿ç”¨å®é™…å‚ä¸è®¡ç®—çš„æœ‰æ•ˆ Masked Pair æ•°é‡
            return mse.sum() / (final_mask.sum() + 1e-8)
        return mse.sum()

# ================= 2. æ ¸å¿ƒå¯¹æ¯”æŸå¤±ï¼šInfoNCE =================

class InfoNCELoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        # ğŸš¨ æ›¿æ¢å›ºå®šçš„ temperatureï¼Œæ”¹ä¸ºå¯å­¦ä¹ çš„ logit_scale
        # åˆå§‹åŒ–ä¸º ln(1/temperature)
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))

    def forward(self, z1, z2):
        # è·¨ GPU å…¨å±€è´Ÿæ ·æœ¬æ”¶é›† (All-Gather)
        if dist.is_initialized():
            z1_list = [torch.zeros_like(z1) for _ in range(dist.get_world_size())]
            z2_list = [torch.zeros_like(z2) for _ in range(dist.get_world_size())]
            dist.all_gather(z1_list, z1)
            dist.all_gather(z2_list, z2)
            
            # ä¿ç•™æœ¬åœ°è®¡ç®—å›¾çš„æ¢¯åº¦
            z1_list[dist.get_rank()] = z1
            z2_list[dist.get_rank()] = z2
            
            z1 = torch.cat(z1_list, dim=0)
            z2 = torch.cat(z2_list, dim=0)

        B = z1.size(0) 
        z = torch.cat([z1, z2], dim=0)  # [2B, D]
        
        # ğŸš¨ é™åˆ¶æœ€å¤§ scale ä¸º 100 (å¯¹åº”æœ€ä½æ¸©åº¦ 0.01)ï¼Œé˜²æ­¢æ—©æœŸæ¢¯åº¦çˆ†ç‚¸
        logit_scale = torch.clamp(self.logit_scale.exp(), max=100.0)
        
        # ğŸš¨ è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¹¶ä¹˜ä»¥å¯å­¦ä¹ çš„ scale (ä»£æ›¿é™¤ä»¥æ¸©åº¦)
        logits = torch.matmul(z, z.T) * logit_scale
        
        # å±è”½å¯¹è§’çº¿ (è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦è®¾ä¸ºæå°å€¼)
        mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)
        logits = logits.masked_fill(mask, -1e9)
        
        # æ„å»ºåˆ†ç±» Target
        targets = torch.cat([
            torch.arange(B, 2 * B, device=z.device),
            torch.arange(0, B, device=z.device)
        ], dim=0)
        
        return F.cross_entropy(logits, targets)

# ================= 3. ç»„åˆå¼•æ“ï¼šCMAE Loss =================

class CMAELoss(nn.Module):
    """
    Contrastive Masked Autoencoder è”åˆæŸå¤±ã€‚
    åŒ…å«ï¼šæ‹‰ä¼¸æµå½¢çš„ InfoNCE + ç‰©ç†ä¿çœŸçš„ Masked_DRMSD
    """
    def __init__(
        self,
        temperature=0.1,
        lambda_contrast=1.0,
        lambda_recon=0.5,
        cutoff=15.0
    ):
        super().__init__()
        self.lambda_contrast = lambda_contrast
        self.lambda_recon = lambda_recon
        
        self.info_nce_loss = InfoNCELoss(temperature=temperature)
        self.drmsd_loss = MaskedDRMSDLoss(reduction='mean', cutoff=cutoff)
        
    # ğŸš¨ æ–°å¢ï¼šæ–¹ä¾¿å¤–éƒ¨ (train_cmae.py) ç›´æ¥è¯»å–å½“å‰æ¸©åº¦è¿›è¡Œ WandB è®°å½•
    @property
    def logit_scale(self):
        return self.info_nce_loss.logit_scale
        
    def forward(
        self,
        z1,
        z2,
        pos_pred_v1,
        pos_true,
        mask_v1,
        batch_idx
    ):
        # 1. è®¡ç®— InfoNCE å¯¹æ¯”æŸå¤±
        contrast_loss = self.info_nce_loss(z1, z2)
        
        # 2. è®¡ç®—ä»…åœ¨ Masked åŒºåŸŸçš„é‡æ„æŸå¤±
        recon_loss = self.drmsd_loss(pos_pred_v1, pos_true, mask_v1, batch_idx)
        
        # 3. ç»„åˆæ€»æŸå¤±
        total_loss = self.lambda_contrast * contrast_loss + self.lambda_recon * recon_loss
        
        return total_loss, contrast_loss, recon_loss