

"""
æŸå¤±å‡½æ•°æ¨¡å—ã€‚
å®ç° D-RMSD (Distance-RMSD) æŸå¤±å’Œ KL æ•£åº¦ï¼Œç”¨äº VAE è®­ç»ƒã€‚

å…³é”®ç‰¹æ€§ï¼š
- D-RMSDï¼šåŸºäºæˆå¯¹è·ç¦»çŸ©é˜µï¼Œä¿è¯ SE(3) ä¸å˜æ€§ï¼Œæ— éœ€å¯¹é½
- KL æ•£åº¦ï¼šÎ²-VAE æ­£åˆ™åŒ–
- Î² é€€ç«ç­–ç•¥
- æ”¯æŒ PyG æ‰¹é‡å›¾å¤„ç†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.utils import to_dense_batch


class DRMSDLoss(nn.Module):
    """
    D-RMSD æŸå¤±å‡½æ•° (å¸¦å±€éƒ¨æˆªæ–­)ã€‚
    åªè®¡ç®—é¢„æµ‹åæ ‡å’ŒçœŸå®åæ ‡åœ¨å±€éƒ¨é‚»åŸŸå†…çš„æˆå¯¹è·ç¦»è¯¯å·®ã€‚
    """
    
    def __init__(self, reduction='mean', cutoff=15.0):
        super().__init__()
        self.reduction = reduction
        self.cutoff = cutoff  # ğŸš¨ æ–°å¢ï¼šæˆªæ–­è·ç¦»ï¼Œå»ºè®® 10.0 ~ 15.0 åŸƒ
        
    def forward(
        self,
        pos_pred,
        pos_true,
        mask=None,
        batch_idx=None
    ):
        if batch_idx is None:
            batch_idx = torch.zeros(pos_pred.size(0), dtype=torch.long, device=pos_pred.device)
        
        # è½¬ä¸ºå¯†é›†çŸ©é˜µ [B, N_max, 3]
        pos_pred_dense, batch_mask = to_dense_batch(pos_pred, batch_idx)
        pos_true_dense, _ = to_dense_batch(pos_true, batch_idx)
        
        # è®¡ç®—å®‰å…¨çš„æˆå¯¹è·ç¦» [B, N_max, N_max]
        D_pred = torch.cdist(pos_pred_dense, pos_pred_dense, p=2.0)
        D_true = torch.cdist(pos_true_dense, pos_true_dense, p=2.0)
        
        mse = (D_pred - D_true) ** 2
        
        # 1. æœ‰æ•ˆèŠ‚ç‚¹æ©ç  (å»é™¤ padding çš„è™šæ‹ŸèŠ‚ç‚¹)
        valid_2d = batch_mask.unsqueeze(1) * batch_mask.unsqueeze(2)
        
        # ================= ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šå¼•å…¥å±€éƒ¨è·ç¦»æˆªæ–­ =================
        # åªæƒ©ç½šçœŸå®è·ç¦»åœ¨ cutoff ä¹‹å†…çš„åŸå­å¯¹ï¼é‡Šæ”¾å…¨å±€ç»“æ„çš„è‡ªç”±åº¦ã€‚
        cutoff_mask = (D_true < self.cutoff).float()
        
        # å»é™¤å¯¹è§’çº¿ï¼ˆåŸå­è‡ªå·±åˆ°è‡ªå·±çš„è·ç¦»ä¸º0ï¼Œä¸ç®—ä½œæœ‰æ•ˆè¯¯å·®é¿å…æ‹‰ä½ meanï¼‰
        eye_mask = 1.0 - torch.eye(D_true.size(1), device=D_true.device).unsqueeze(0)
        
        # ç»„åˆæˆæœ€ç»ˆçš„åŸºç¡€ mask
        base_mask = valid_2d * cutoff_mask * eye_mask
        # ===============================================================
        
        if mask is not None:
            mask_dense, _ = to_dense_batch(mask, batch_idx)
            mask_2d = mask_dense.unsqueeze(1) * mask_dense.unsqueeze(2)
            final_mask = mask_2d * base_mask
        else:
            final_mask = base_mask
        
        mse = mse * final_mask
        
        if self.reduction == 'mean':
            # åˆ†æ¯ä½¿ç”¨å®é™…å‚ä¸è®¡ç®—çš„æœ‰æ•ˆ Pair æ•°é‡
            return mse.sum() / (final_mask.sum() + 1e-8)
        return mse.sum()


class KLLoss(nn.Module):
    """
    çœŸå®çš„ KL æ•£åº¦è®¡ç®—ï¼ˆç”¨äºçœŸå®æ—¥å¿—è®°å½•ï¼‰ã€‚
    """
    def __init__(self, reduction='batchmean'):
        super().__init__()
        self.reduction = reduction
        
    def forward(self, mu, logvar):
        kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())
        
        if self.reduction == 'batchmean':
            # çœŸå®ã€æœªç¨€é‡Šçš„æ€» KL æ•£åº¦
            return kl.sum(dim=-1).mean()
        elif self.reduction == 'mean':
            return kl.mean()
        elif self.reduction == 'sum':
            return kl.sum()
        else:
            return kl


class BetaScheduler:
    """
    Î² é€€ç«è°ƒåº¦å™¨ã€‚
    ç”¨äº Î²-VAEï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ  Î² çš„å€¼ã€‚
    
    æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ï¼š
    - linear: çº¿æ€§å¢é•¿
    - cyclic: å‘¨æœŸé€€ç«
    - step: é˜¶æ¢¯å¼å¢é•¿
    """
    
    def __init__(
        self,
        beta_start=0.0,
        beta_end=1.0,
        warmup_steps=10000,
        schedule_type='linear',
        cycle_length=None
    ):
        self.beta_start = beta_start
        self.beta_end = beta_end
        self.warmup_steps = warmup_steps
        self.schedule_type = schedule_type
        self.cycle_length = cycle_length if cycle_length is not None else warmup_steps * 2
        self.step = 0
        
    def update(self):
        """æ›´æ–°å¹¶è¿”å›å½“å‰çš„ Î² å€¼ã€‚"""
        self.step += 1
        return self.get_beta()
        
    def get_beta(self):
        """è·å–å½“å‰çš„ Î² å€¼ï¼Œä¸æ›´æ–°æ­¥æ•°ã€‚"""
        if self.schedule_type == 'linear':
            if self.step < self.warmup_steps:
                beta = self.beta_start + (self.beta_end - self.beta_start) * (self.step / self.warmup_steps)
            else:
                beta = self.beta_end
                
        elif self.schedule_type == 'cyclic':
            # å‘¨æœŸé€€ç«ï¼šæ¯ cycle_length æ­¥é‡å¤ä¸€æ¬¡
            cycle_pos = self.step % self.cycle_length
            if cycle_pos < self.warmup_steps:
                beta = self.beta_start + (self.beta_end - self.beta_start) * (cycle_pos / self.warmup_steps)
            else:
                beta = self.beta_end
                
        elif self.schedule_type == 'step':
            # é˜¶æ¢¯å¼ï¼šåœ¨å›ºå®šæ­¥æ•°è·³è·ƒ
            if self.step < self.warmup_steps // 4:
                beta = self.beta_start
            elif self.step < self.warmup_steps // 2:
                beta = self.beta_start + (self.beta_end - self.beta_start) * 0.25
            elif self.step < 3 * self.warmup_steps // 4:
                beta = self.beta_start + (self.beta_end - self.beta_start) * 0.5
            elif self.step < self.warmup_steps:
                beta = self.beta_start + (self.beta_end - self.beta_start) * 0.75
            else:
                beta = self.beta_end
        else:
            raise ValueError(f"Unknown schedule type: {self.schedule_type}")
            
        return beta


class VAELoss(nn.Module):
    """
    å®Œæ•´çš„ VAE æŸå¤±å‡½æ•°ã€‚
    ç»„åˆ D-RMSD é‡å»ºæŸå¤±å’Œå¸¦ Free Bits çš„ KL æ•£åº¦æ­£åˆ™åŒ–ã€‚
    """
    
    def __init__(
        self,
        beta=0.1,
        recon_reduction='mean',
        kl_reduction='batchmean',
        free_bits=2.0  # ğŸš¨ åœ¨è¿™é‡Œå¼•å…¥ free_bits
    ):
        super().__init__()
        self.beta = beta
        self.free_bits = free_bits
        self.drmsd_loss = DRMSDLoss(reduction=recon_reduction)
        self.kl_loss = KLLoss(reduction=kl_reduction)
        
    def forward(
        self,
        pos_pred,
        pos_true,
        mu,
        logvar,
        mask=None,
        batch_idx=None
    ):
        # 1. è®¡ç®—é‡æ„è¯¯å·®
        recon_loss = self.drmsd_loss(pos_pred, pos_true, mask, batch_idx)
        
        # 2. è®¡ç®—çœŸå®çš„ KL æ•£åº¦ï¼ˆç”¨äºåœ¨ WandB ä¸Šé€æ˜ç›‘æ§ï¼ï¼‰
        raw_kl = self.kl_loss(mu, logvar)
        
        # 3. ğŸš¨ æ ¸å¿ƒé­”æ³•ï¼šè®¡ç®—ç”¨äºåå‘ä¼ æ’­çš„æˆªæ–­ KL (Hinge Loss)
        # ä¼˜åŒ–å™¨åªä¼šçœ‹åˆ°è¿™ä¸ª clamped_klï¼Œæ‰€ä»¥ä½äº free_bits æ—¶æ²¡æœ‰æ¢¯åº¦
        clamped_kl = torch.clamp(raw_kl - self.free_bits, min=0.0)
        
        # 4. ç»„è£…æ€» Lossï¼ˆç»™æ¨¡å‹ä¼˜åŒ–çš„çœŸæ­£ç›®æ ‡ï¼‰
        total_loss = recon_loss + self.beta * clamped_kl
        
        # ğŸš¨ æ³¨æ„çœ‹è¿”å›å€¼ï¼šæˆ‘ä»¬è¿”å› total_loss ç»™ä¼˜åŒ–å™¨ï¼Œä½†è¿”å› raw_kl ç»™ WandBï¼
        return total_loss, recon_loss, raw_kl


class CoordinateDecoder(nn.Module):
    """
    ç®€å•çš„åæ ‡è§£ç å™¨ï¼Œä»ç‰¹å¾å‘é‡é¢„æµ‹åŸå­åæ ‡ã€‚
    ç”¨äº VAE è§£ç å™¨éƒ¨åˆ†ã€‚
    """
    
    def __init__(self, hidden_dim, num_layers=2):
        super().__init__()
        layers = []
        in_dim = hidden_dim
        
        for _ in range(num_layers - 1):
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.SiLU())
            in_dim = hidden_dim
            
        layers.append(nn.Linear(in_dim, 3))
        self.mlp = nn.Sequential(*layers)
        
    def forward(self, x):
        """
        å‚æ•°:
            x: [N, hidden_dim] åŸå­ç‰¹å¾
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»
        """
        return self.mlp(x)

