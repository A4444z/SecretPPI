
import os
import pickle
import lmdb
import torch
import numpy as np
import json
from torch_geometric.data import Dataset, Data
from torch_cluster import radius_graph, knn_graph
from torch_scatter import scatter_add
from typing import Optional, List, Tuple

from src.utils.geometry import GaussianRBF, get_random_rotation_matrix, apply_rotation


class GlueVAEDataset(Dataset):
    """
    GlueVAE è›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢æ•°æ®é›†ç±»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ã€‚
    
    è¯¥ç±»ç»§æ‰¿è‡ª PyG çš„ Datasetï¼Œè´Ÿè´£ä» LMDB è¯»å–å¤„ç†å¥½çš„ç•Œé¢æ•°æ®ï¼Œå¹¶åŠ¨æ€æ„å»ºå‡ ä½•å›¾ç»“æ„ã€‚
    
    ä¸»è¦ä¼˜åŒ–ï¼š
    - Dynamic Patch Sampling: å¯¹å¤§ç•Œé¢è¿›è¡Œéšæœºè¡¥ä¸é‡‡æ ·ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
    - ä¼˜åŒ–å›¾æ„å»º: é™åˆ¶æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§é‚»å±…æ•°ï¼Œé˜²æ­¢è¾¹æ•°çˆ†ç‚¸
    - é²æ£’çš„å‘é‡ç‰¹å¾è®¡ç®—: å¤„ç†å­¤ç«‹åŸå­æƒ…å†µ
    - å¢å¼ºçš„å…ƒæ•°æ®: ä¾¿äºè°ƒè¯•å’Œåˆ†æ
    """
    
    def __init__(
        self, 
        root: str, 
        split: str = 'train', 
        transform=None, 
        pre_transform=None, 
        lmdb_path: Optional[str] = None,
        max_atoms: int = 1000,
        patch_radius: float = 15.0,
        max_num_neighbors: int = 0,
        num_fps_points: int = 5,
        exclude_pdb_json: Optional[str] = None,
        random_rotation: bool = True,
        max_samples: Optional[int] = None,
        cutoff_radius: float = 8.0
    ):
        """
        å‚æ•°:
            root: æ•°æ®é›†æ ¹ç›®å½•ã€‚
            split: 'train', 'val', æˆ– 'test'ã€‚ç”¨äºå†³å®šæ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼ºã€‚
            lmdb_path: LMDB æ•°æ®åº“è·¯å¾„ã€‚å¦‚æœä¸º Noneï¼Œåˆ™é»˜è®¤ä½¿ç”¨ root/processed_lmdbã€‚
            max_atoms: è§¦å‘è¡¥ä¸é‡‡æ ·çš„æœ€å¤§åŸå­æ•°ï¼Œé»˜è®¤1000ã€‚
            patch_radius: è¡¥ä¸é‡‡æ ·çš„åŠå¾„é˜ˆå€¼ï¼Œå•ä½Ã…ï¼Œé»˜è®¤15.0ã€‚
            max_num_neighbors: æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§é‚»å±…æ•°ï¼Œé˜²æ­¢è¾¹æ•°çˆ†ç‚¸ï¼Œé»˜è®¤0ã€‚
            num_fps_points: ä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·(FPS)ç”Ÿæˆçš„å€™é€‰ä¸­å¿ƒæ•°é‡ï¼Œé»˜è®¤5ã€‚
            exclude_pdb_json: åŒ…å«éœ€è¦æ’é™¤çš„PDB IDçš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚CASF-2016ï¼‰ã€‚
            max_samples: æœ€å¤šåŠ è½½å¤šå°‘ä¸ªæ ·æœ¬ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨åŠ è½½ã€‚
        """
        self.lmdb_path = lmdb_path or os.path.join(root, "processed_lmdb")
        self.split = split
        self.max_atoms = max_atoms
        self.patch_radius = patch_radius
        self.max_num_neighbors = max_num_neighbors
        self.num_fps_points = num_fps_points
        self.max_samples = max_samples
        self._keys: Optional[List[bytes]] = None
        self._env: Optional[lmdb.Environment] = None
        
        # åŠ è½½éœ€è¦æ’é™¤çš„PDB ID
        self.exclude_pdb_ids = set()
        if exclude_pdb_json is not None and os.path.exists(exclude_pdb_json):
            with open(exclude_pdb_json, 'r') as f:
                exclude_data = json.load(f)
                if 'all_pdb_ids' in exclude_data:
                    self.exclude_pdb_ids = set(pdb_id.lower() for pdb_id in exclude_data['all_pdb_ids'])
                    print(f"å·²åŠ è½½ {len(self.exclude_pdb_ids)} ä¸ªéœ€æ’é™¤çš„PDB ID")
        
        # æ–°å¢ï¼šæ˜¯å¦éšæœºæ—‹è½¬
        self.random_rotation = random_rotation
        
        # ç”¨äºç»´æŠ¤æ¯ä¸ªæ ·æœ¬çš„é‡‡æ ·çŠ¶æ€
        self._sample_states = {}
        
        # å‡ ä½•è®¡ç®—å·¥å…·ï¼šé«˜æ–¯å¾„å‘åŸºå‡½æ•° (RBF)
        self.rbf = GaussianRBF(n_rbf=16, cutoff=self.cutoff_radius, start=0.0)
        
        super().__init__(root, transform, pre_transform)
    
    def _process(self):
        """
        è¦†ç›–å¹¶ç¦ç”¨ PyG é»˜è®¤çš„ _process é€»è¾‘ã€‚
        å› ä¸ºæˆ‘ä»¬çš„æ•°æ®åœ¨ LMDB ä¸­ç›´æ¥è¯»å–ï¼Œä¸éœ€è¦ PyG åœ¨ processed/ ä¸‹ç”Ÿæˆæ— ç”¨çš„ .pt æ–‡ä»¶ã€‚
        è¿™å½»åº•æ ¹é™¤äº†å¤šå¡å¹¶å‘è¯»å†™ NFS å¯¼è‡´çš„ _pickle.UnpicklingErrorï¼
        """
        pass

    def __getstate__(self):
        """
        è¿™ä¸ªæ–¹æ³•æå…¶é‡è¦ï¼
        å½“ PyTorch DataLoader ä½¿ç”¨å¤šè¿›ç¨‹ (num_workers > 0) æ—¶ï¼Œ
        å®ƒä¼šæŠŠ Dataset åºåˆ—åŒ–å‘ç»™å­è¿›ç¨‹ã€‚
        å¦‚æœä¸æŠŠ _env ç½®ä¸º Noneï¼Œå­è¿›ç¨‹ä¼šå…±äº«ä¸»è¿›ç¨‹çš„ LMDB å¥æŸ„å¯¼è‡´æ­»é”ï¼
        """
        state = self.__dict__.copy()
        state['_env'] = None  # å¼ºåˆ¶å­è¿›ç¨‹è‡ªå·±é‡æ–°åˆå§‹åŒ– LMDB è¿æ¥
        return state

    @property
    def raw_file_names(self) -> List[str]:
        return []
    
    @property
    def processed_file_names(self) -> List[str]:
        return []
    
    def download(self):
        pass
    
    def process(self):
        pass
    
    def _connect_db(self):
        import os
        current_pid = os.getpid()
        
        # ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœç¯å¢ƒæœªåˆå§‹åŒ–ï¼Œæˆ–è€…å‘ç°è‡ªå·±æ˜¯å­è¿›ç¨‹ï¼ˆPID å˜äº†ï¼‰
        if getattr(self, '_env', None) is None or getattr(self, '_env_pid', None) != current_pid:
            import lmdb
            
            # (å¯é€‰) å¦‚æœå­è¿›ç¨‹ç»§æ‰¿äº†æ—§ç¯å¢ƒï¼Œæˆ‘ä»¬ä¸å…³é—­å®ƒï¼ˆé˜²æ­¢å½±å“å…¶ä»–è¿›ç¨‹ï¼‰ï¼Œç›´æ¥è¦†ç›–
            self._env = lmdb.open(
                self.lmdb_path,
                readonly=True,
                lock=False,       # æŠµå¾¡ NFS é”å´©æºƒ
                readahead=False,  # é˜²æ­¢é¢„è¯»å¯¼è‡´å†…å­˜æ³„æ¼
                meminit=False,    # åŠ é€Ÿè¯»å–
                max_readers=1024  # å¢å¤§å¹¶å‘ä¸Šé™
            )
            self._env_pid = current_pid  # è®°ä½å½“å‰ç¯å¢ƒæ˜¯è°å¼€çš„
    
    def _load_keys(self): 
        """ä»æ•°æ®åº“ä¸­åŠ è½½ Keysï¼Œæ”¯æŒå®Œæ•´ç¼“å­˜ä¸å¿«é€Ÿè°ƒè¯•æˆªæ–­ã€‚""" 
        self._connect_db() 
         
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„ 
        cache_path = os.path.join(self.lmdb_path, f"keys_cache_{self.split}.pkl") 
         
        # ================= æƒ…æ™¯ 1ï¼šå…¨é‡æ¨¡å¼ä¸”å­˜åœ¨ç¼“å­˜ -> ç§’é€Ÿè¯»å– ================= 
        if self._keys is None and self.max_samples is None and os.path.exists(cache_path): 
            print("\n" + "="*60) 
            print(f"ğŸš€ [CACHE HIT] å‘ç°å…¨é‡ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨ç§’é€ŸåŠ è½½ï¼") 
            print(f"ğŸ“‚ è·¯å¾„: {cache_path}") 
            with open(cache_path, 'rb') as f: 
                self._keys = pickle.load(f) 
            print(f"âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½ {len(self._keys)} ä¸ªæ ·æœ¬ï¼å¯åŠ¨èµ·é£ï¼") 
            print("="*60 + "\n") 
            return 
 
        # ================= æƒ…æ™¯ 2ï¼šæ— ç¼“å­˜ æˆ– å¤„äºé™åˆ¶æ•°é‡çš„è°ƒè¯•æ¨¡å¼ ================= 
        if self._keys is None: 
            self._keys = [] 
            print("\n" + "="*60) 
            if self.max_samples is not None: 
                print(f"âš ï¸ [DEBUG MODE] å½“å‰é™åˆ¶æœ€å¤§è¯»å–æ•°é‡: {self.max_samples}") 
            else: 
                print("â³ [CACHE MISS] æœªæ‰¾åˆ°ç¼“å­˜ï¼Œæ­£åœ¨éå† LMDB æ•°æ®åº“...") 
                print("   ï¼ˆå› ä¸ºèµ°ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿï¼Œè¿™å¯èƒ½éœ€è¦5 hï¼Œè¯·å–100æ¯å’–å•¡ï¼‰") 
             
            with self._env.begin() as txn: 
                cursor = txn.cursor() 
                 
                for k, _ in cursor: 
                    # 1. é»‘åå•è¿‡æ»¤ (ä¾‹å¦‚ CASF-2016) 
                    if self.exclude_pdb_ids: 
                        try: 
                            key_str = k.decode('utf-8') 
                            pdb_id = key_str.split('|')[0].lower() 
                            if pdb_id in self.exclude_pdb_ids: 
                                continue # å‘½ä¸­é»‘åå•ï¼Œç›´æ¥è·³è¿‡ 
                        except: 
                            continue # æ ¼å¼é”™è¯¯è·³è¿‡ 
                     
                    # 2. é€šè¿‡ç­›é€‰ï¼ŒåŠ å…¥åˆ—è¡¨ 
                    self._keys.append(k) 
                     
                    # 3. æˆªæ–­åˆ¤æ–­ï¼šä¸€æ—¦å‡‘å¤Ÿäº†æˆ‘ä»¬éœ€è¦çš„æ•°é‡ï¼Œç«‹åˆ»æ€æ¡Œå­èµ°äººï¼ 
                    if self.max_samples is not None and len(self._keys) >= self.max_samples: 
                        print(f"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ ({self.max_samples})ï¼Œæå‰ç»ˆæ­¢éå†ï¼") 
                        break 
             
            print(f"âœ… æœ¬æ¬¡å®é™…éå†åŠ è½½äº† {len(self._keys)} ä¸ªæ ·æœ¬ã€‚") 
             
            # ================= æƒ…æ™¯ 3ï¼šå…¨é‡æ¨¡å¼ä¸‹ä¿å­˜ç¼“å­˜ ================= 
            if self.max_samples is None: 
                print(f"ğŸ’¾ [SAVING CACHE] æ­£åœ¨å°†å…¨é‡ç›®å½•ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶...") 
                with open(cache_path, 'wb') as f: 
                    pickle.dump(self._keys, f) 
                print(f"ğŸ‰ ç¼“å­˜ä¿å­˜æˆåŠŸï¼æ–‡ä»¶ä½ç½®: {cache_path}") 
                print(f"   ä¸‹ä¸€æ¬¡å¯åŠ¨è®­ç»ƒå°†åªéœ€ 1 ç§’é’Ÿï¼") 
            else: 
                print("ğŸš« [NO CACHE] æç¤ºï¼šå½“å‰ä¸ºå±€éƒ¨è°ƒè¯•æ¨¡å¼ï¼Œä¸ºäº†é˜²æ­¢ç¼“å­˜è¢«æ±¡æŸ“ï¼Œæœ¬æ¬¡ã€ä¸ä¿å­˜ã€‘ç¼“å­˜ã€‚") 
            print("="*60 + "\n")
    def len(self) -> int:
        """è¿”å›æ•°æ®é›†æ ·æœ¬æ€»æ•°ã€‚"""
        self._load_keys()
        return len(self._keys)
    
    def _farthest_point_sampling(
        self,
        points: torch.Tensor,
        num_points: int
    ) -> torch.Tensor:
        """
        æœ€è¿œç‚¹é‡‡æ ·(FPS)ï¼šä»ç‚¹é›†ä¸­é€‰æ‹©æœ€è¿œçš„num_pointsä¸ªç‚¹ã€‚
        
        å‚æ•°:
            points: ç‚¹åæ ‡ [N, 3]
            num_points: è¦é€‰æ‹©çš„ç‚¹æ•°
        
        è¿”å›:
            selected_indices: é€‰ä¸­ç‚¹çš„ç´¢å¼• [num_points]
        """
        N = points.size(0)
        if N <= num_points:
            return torch.arange(N, device=points.device)
        
        selected_indices = []
        # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªç‚¹
        idx = torch.randint(0, N, (1,), device=points.device).item()
        selected_indices.append(idx)
        
        # è®¡ç®—æ‰€æœ‰ç‚¹åˆ°å·²é€‰ç‚¹çš„æœ€å°è·ç¦»
        dists = torch.norm(points - points[idx:idx+1], dim=1)
        
        for _ in range(1, num_points):
            # é€‰æ‹©è·ç¦»æœ€è¿œçš„ç‚¹
            idx = torch.argmax(dists).item()
            selected_indices.append(idx)
            
            # æ›´æ–°æœ€å°è·ç¦»
            new_dists = torch.norm(points - points[idx:idx+1], dim=1)
            dists = torch.min(dists, new_dists)
        
        return torch.tensor(selected_indices, device=points.device)
    
    def _get_or_create_sample_state(
        self,
        key: bytes,
        pos: torch.Tensor,
        mask_interface: torch.Tensor
    ) -> Tuple[List[int], int]:
        """
        è·å–æˆ–åˆ›å»ºæ ·æœ¬çš„é‡‡æ ·çŠ¶æ€ã€‚
        ã€å·¥ç¨‹é˜²çº¿ã€‘ï¼šåŠ å…¥æç«¯å¼‚å¸¸æ ·æœ¬å…œåº•ä¿æŠ¤ã€‚
        """
        key_str = key.decode('utf-8')
        
        if key_str not in self._sample_states:
            # ç¬¬ä¸€æ¬¡è®¿é—®è¯¥æ ·æœ¬ï¼Œç”ŸæˆFPSå€™é€‰ä¸­å¿ƒ
            interface_indices = torch.where(mask_interface == 1)[0]
            
            # ================= ğŸš¨ å·¥ç¨‹é˜²çº¿ =================
            # å¦‚æœæ²¡æœ‰ä»»ä½•ç•Œé¢åŸå­ï¼Œé€€åŒ–ä¸ºä»å…¨ä½“åŸå­ä¸­é€‰ä¸­å¿ƒ
            if len(interface_indices) == 0:
                interface_indices = torch.arange(pos.size(0), device=pos.device)
            # ===============================================
            
            if len(interface_indices) < self.num_fps_points:
                candidate_indices = interface_indices
            else:
                interface_pos = pos[interface_indices]
                fps_indices_in_interface = self._farthest_point_sampling(
                    interface_pos,
                    self.num_fps_points
                )
                candidate_indices = interface_indices[fps_indices_in_interface]
            
            cand_list = candidate_indices.tolist()
            if not cand_list:
                cand_list = [0] if pos.size(0) > 0 else []
                
            self._sample_states[key_str] = {
                'candidate_centers': cand_list,
                'current_index': 0
            }
        
        state = self._sample_states[key_str]
        return state['candidate_centers'], state['current_index']
    
    def _update_sample_state(self, key: bytes):
        """
        æ›´æ–°æ ·æœ¬çš„é‡‡æ ·çŠ¶æ€ï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå€™é€‰ä¸­å¿ƒã€‚
        """
        key_str = key.decode('utf-8')
        if key_str in self._sample_states:
            state = self._sample_states[key_str]
            if len(state['candidate_centers']) > 0:
                state['current_index'] = (state['current_index'] + 1) % len(state['candidate_centers'])

    def _dynamic_patch_sampling(
        self,
        key: bytes,
        pos: torch.Tensor,
        z: torch.Tensor,
        residue_index: torch.Tensor,
        is_ligand: torch.Tensor,
        mask_interface: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, bool, int]:
        """
        åŠ¨æ€è¡¥ä¸é‡‡æ ·ï¼šå¦‚æœåŸå­æ•°è¶…è¿‡ max_atomsï¼Œåˆ™é‡‡æ ·ä»¥ç•Œé¢ä¸ºä¸­å¿ƒçš„å±€éƒ¨è¡¥ä¸ã€‚
        
        ç­–ç•¥ï¼šä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·(FPS)ç³»ç»Ÿæ€§åœ°è¦†ç›–æ•´ä¸ªç•Œé¢ã€‚
              å…ˆé‡‡æ · Patchï¼Œå†æ—‹è½¬ Patchï¼Œè¿™æ ·è®¡ç®—é‡æ›´å°ã€‚
        
        å‚æ•°:
            key: æ•°æ®é”®ï¼Œç”¨äºç»´æŠ¤é‡‡æ ·çŠ¶æ€
            pos: åŸå­åæ ‡ [N, 3]
            z: åŸå­åºæ•° [N]
            residue_index: æ®‹åŸºç´¢å¼• [N]
            is_ligand: å—ä½“/é…ä½“æ ‡è®° [N]
            mask_interface: ç•Œé¢æ ‡è®° [N]
        
        è¿”å›:
            (pos, z, residue_index, is_ligand, mask_interface, is_patched, patch_index)
        """
        N = pos.size(0)
        is_patched = False
        patch_index = 0
        
        if N <= self.max_atoms:
            return pos, z, residue_index, is_ligand, mask_interface, is_patched, patch_index
        
        is_patched = True
        
        # è·å–æˆ–åˆ›å»ºè¯¥æ ·æœ¬çš„FPSå€™é€‰ä¸­å¿ƒ
        candidate_centers, current_index = self._get_or_create_sample_state(
            key, pos, mask_interface
        )
        
        patch_index = current_index
        center_idx = candidate_centers[current_index]
        
        # æ›´æ–°é‡‡æ ·çŠ¶æ€ï¼Œä¸‹æ¬¡ä½¿ç”¨ä¸‹ä¸€ä¸ªä¸­å¿ƒ
        self._update_sample_state(key)
        
        # è®¡ç®—æ‰€æœ‰åŸå­åˆ°ä¸­å¿ƒçš„è·ç¦»
        center_pos = pos[center_idx:center_idx+1]  # [1, 3]
        dist_to_center = torch.norm(pos - center_pos, dim=1)  # [N]
        
        # ä¿ç•™è·ç¦»å°äº patch_radius çš„åŸå­
        keep_mask = dist_to_center < self.patch_radius
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›åŸå­
        if keep_mask.sum() < 100:
            # å¦‚æœå¤ªå°‘ï¼Œæ”¾å®½é˜ˆå€¼
            keep_mask = dist_to_center < self.patch_radius * 1.5
        
        # è£å‰ªæ‰€æœ‰æ•°ç»„
        pos = pos[keep_mask]
        z = z[keep_mask]
        residue_index = residue_index[keep_mask]
        is_ligand = is_ligand[keep_mask]
        mask_interface = mask_interface[keep_mask]
        
        return pos, z, residue_index, is_ligand, mask_interface, is_patched, patch_index
    
    def _build_optimized_graph(
        self,
        pos: torch.Tensor
    ) -> torch.Tensor:
        """
        ä¼˜åŒ–çš„å›¾æ„å»ºï¼ˆå‘é‡åŒ–ç‰ˆï¼‰ï¼š
        ç­–ç•¥ï¼šå…ˆç”¨ KNN é™åˆ¶æœ€å¤§é‚»å±…æ•°ï¼Œå†ç”¨åŠå¾„ç­›é€‰ã€‚
        è¿™æ ·æ—¢é™åˆ¶äº†è¾¹æ•°ï¼Œåˆä¿è¯äº†ç‰©ç†è·ç¦»ï¼Œä¸”å…¨ç¨‹ GPU/C++ åŠ é€Ÿã€‚
        """
        if self.max_num_neighbors <= 0:
            return radius_graph(pos, r=self.cutoff_radius, loop=False)
        
        # 1. å…ˆæ‰¾æœ€è¿‘çš„ max_num_neighbors (e.g. 32) ä¸ªé‚»å±…
        # flow='target_to_source' æ˜¯ PyG é»˜è®¤çš„æ¶ˆæ¯ä¼ é€’æ–¹å‘
        edge_index = knn_graph(pos, k=self.max_num_neighbors, loop=False, flow='target_to_source')
        
        # 2. è®¡ç®—è¿™äº›è¾¹çš„è·ç¦»
        row, col = edge_index
        dist = torch.norm(pos[row] - pos[col], p=2, dim=1)
        
        # 3. å†æ¬¡åº”ç”¨åŠå¾„é˜ˆå€¼ (4.5A) è¿›è¡Œè£å‰ª
        mask = dist < self.cutoff_radius
        edge_index = edge_index[:, mask]
        
        return edge_index
    
    def _compute_robust_vector_features(
        self,
        pos: torch.Tensor,
        edge_index: torch.Tensor,
        is_covalent: torch.Tensor
    ) -> torch.Tensor:
        """
        é²æ£’çš„å‘é‡ç‰¹å¾è®¡ç®—ï¼šå¤„ç†å­¤ç«‹åŸå­æƒ…å†µã€‚
        
        å‚æ•°:
            pos: åŸå­åæ ‡ [N, 3]
            edge_index: è¾¹ç´¢å¼• [2, E]
            is_covalent: å…±ä»·è¾¹æ ‡è®° [E]
        
        è¿”å›:
            vector_features: å‘é‡ç‰¹å¾ [N, 3]
        """
        row, col = edge_index
        
        # åªä¿ç•™å…±ä»·è¾¹
        mask_cov = is_covalent
        row_cov = row[mask_cov]
        col_cov = col[mask_cov]
        
        N = pos.size(0)
        vector_features = torch.zeros(N, 3, device=pos.device)
        
        if len(row_cov) > 0:
            # è®¡ç®—ç›¸å¯¹å‘é‡ (j æŒ‡å‘ i)
            vec_diff = pos[row_cov] - pos[col_cov]  # [Ec, 3]
            
            # å°†ç›¸å¯¹å‘é‡ç´¯åŠ åˆ°ç›®æ ‡åŸå­èŠ‚ç‚¹ä¸Š
            vector_features = scatter_add(vec_diff, col_cov, dim=0, dim_size=N)  # [N, 3]
        
        # å¤„ç†å­¤ç«‹åŸå­ï¼šæ·»åŠ å¾®å°éšæœºå‘é‡é¿å…é›¶å‘é‡
        zero_mask = (vector_features.norm(dim=1) < 1e-8)
        if zero_mask.any():
            # æ·»åŠ å¾®å°çš„éšæœºå‘é‡
            random_vec = torch.randn(zero_mask.sum(), 3, device=pos.device) * 1e-4
            vector_features[zero_mask] = random_vec
        
        return vector_features
    
    def get(self, idx: int) -> Data:
        """
        è·å–ç´¢å¼•ä¸º idx çš„æ•°æ®æ ·æœ¬å¹¶æ„å»º PyG Data å¯¹è±¡ã€‚
        ã€ç§‘å­¦é˜²çº¿ã€‘ï¼šé’ˆå¯¹ Train é›†è¿›è¡Œå¼‚å¸¸è¿‡æ»¤é‡é‡‡æ ·ï¼Œé’ˆå¯¹ Val/Test è®°å½•å¹¶æ‰§è¡Œå…œåº•ã€‚
        """
        import random
        self._connect_db()
        if self._keys is None:
            self._load_keys()
            
        # ================= ğŸš¨ ç§‘å­¦é˜²çº¿ä¸æ—¥å¿—ç³»ç»Ÿ =================
        # Codex å»ºè®® 1ï¼šåªåœ¨è®­ç»ƒé›†å…è®¸é‡è¯•ï¼ŒéªŒè¯/æµ‹è¯•é›†é‡åˆ°ç›´æ¥è·³è¿‡é‡é‡‡æ ·ï¼Œèµ°åº•å±‚å…œåº•
        max_retries = 10 if self.split == 'train' else 1
        
        for attempt in range(max_retries):
            key = self._keys[idx]
            with self._env.begin() as txn:
                byte_data = txn.get(key)
                data_dict = pickle.loads(byte_data)
                
            # 1. æå–åŸºç¡€æ•°ç»„
            pos = torch.from_numpy(data_dict['pos']).float()
            z = torch.from_numpy(data_dict['z']).long()
            residue_index = torch.from_numpy(data_dict['residue_index']).long()
            res_keys = data_dict['residue_keys']
            meta = data_dict['meta']
            chain_a, chain_b = meta['chains']
            
            # 2. ç¡®å®š is_ligand
            res_to_batch = []
            for rk in res_keys:
                cid = rk[0]
                if cid == chain_a:
                    res_to_batch.append(0)
                else:
                    res_to_batch.append(1)
            
            res_to_batch_tensor = torch.tensor(res_to_batch, dtype=torch.long)
            is_ligand = res_to_batch_tensor[residue_index]
            
            # 3. ç•Œé¢æ©ç  (Mask Interface)
            mask_interface = torch.zeros(pos.size(0), dtype=torch.float)
            mask_a = (is_ligand == 0)
            mask_b = (is_ligand == 1)
            
            is_valid_sample = True
            if mask_a.any() and mask_b.any():
                pos_a = pos[mask_a]
                pos_b = pos[mask_b]
                
                dist_mat = torch.cdist(pos_a, pos_b)
                min_dist_a, _ = dist_mat.min(dim=1)
                min_dist_b, _ = dist_mat.min(dim=0)
                
                interface_a = (min_dist_a < 4.0).float()
                interface_b = (min_dist_b < 4.0).float()
                
                # åˆ¤æ–­æ˜¯ä¸æ˜¯å¥‡è‘©æ ·æœ¬ï¼ˆä¾‹å¦‚è·ç¦»è¿‡è¿œï¼‰
                if interface_a.sum() == 0 or interface_b.sum() == 0:
                    is_valid_sample = False
                else:
                    mask_interface[mask_a] = interface_a
                    mask_interface[mask_b] = interface_b
            else:
                is_valid_sample = False  # æ®‹ç¼º PDB
                
            if is_valid_sample:
                break  # æŠ½åˆ°äº†å¥½æ•°æ®ï¼Œç›´æ¥æ‰“ç ´å¾ªç¯ï¼
            else:
                if attempt < max_retries - 1:
                    # å¦‚æœè¿˜æ²¡è€—å°½é‡è¯•æ¬¡æ•°ï¼Œé‡æ–°éšæœºæŠ½å¡
                    idx = random.randint(0, len(self._keys) - 1)

        # Codex å»ºè®® 2ï¼šé‡è¯•è€—å°½ï¼ˆæˆ–éªŒè¯é›†ä¸é‡è¯•ï¼‰æ—¶ï¼Œæ‰“å°è¯¦ç»†çš„è¿½æº¯æ—¥å¿—
        if not is_valid_sample:
            pdb_id = meta.get('pdb_id', 'unknown')
            print(f"\n[WARNING] è§¦å‘å¼‚å¸¸æ•°æ®å…œåº•æœºåˆ¶ | Split: {self.split} | PDB: {pdb_id} | Key: {key.decode('utf-8')} | Attempt: {attempt+1}/{max_retries}")
        # ===============================================================

        # 4. Dynamic Patch Sampling
        is_patched = False
        patch_index = 0
        original_num_nodes = pos.size(0)
        
        # å³ä½¿æ˜¯åæ•°æ®ï¼ˆéªŒè¯é›†ï¼Œæˆ–è€…è®­ç»ƒé›†è¿æŠ½ 10 æ¬¡é»‘åº•ï¼‰ï¼Œåº•å±‚çš„å·¥ç¨‹é˜²çº¿ä¹Ÿèƒ½ä¿è¯æ­¤å‡½æ•°ä¸å´©
        pos, z, residue_index, is_ligand, mask_interface, is_patched, patch_index = self._dynamic_patch_sampling(
            key, pos, z, residue_index, is_ligand, mask_interface
        )

        # =========== æ ¸å¿ƒä¿®å¤ï¼šåæ ‡å»ä¸­å¿ƒåŒ– ===========
        if pos.shape[0] > 0:
            pos_center = pos.mean(dim=0, keepdim=True)
            pos = pos - pos_center
        # ============================================
        # 5. æ•°æ®å¢å¼º (éšæœºæ—‹è½¬) - åœ¨Patch Samplingä¹‹å
        if self.split == 'train' and self.random_rotation:
            rot_mat = get_random_rotation_matrix()
            pos = apply_rotation(pos, rot_mat)
        
        # 6. ä¼˜åŒ–å›¾ç»“æ„æ„å»º
        edge_index = self._build_optimized_graph(pos)
        row, col = edge_index
        
        # 7. è®¡ç®—è¾¹çš„æ¬§æ°è·ç¦»å’Œç±»å‹
        diff = pos[row] - pos[col]
        dist = torch.norm(diff, p=2, dim=-1)
        
        is_covalent = dist < 1.7
        same_chain = (is_ligand[row] == is_ligand[col])
        
        edge_type = torch.zeros(edge_index.size(1), 3, dtype=torch.float)
        
        mask_type0 = is_covalent
        edge_type[mask_type0, 0] = 1.0
        
        mask_type1 = (~is_covalent) & same_chain
        edge_type[mask_type1, 1] = 1.0
        
        mask_type2 = (~is_covalent) & (~same_chain)
        edge_type[mask_type2, 2] = 1.0
        
        rbf_feat = self.rbf(dist)
        edge_attr = torch.cat([edge_type, rbf_feat], dim=-1)
        
        # 8. é²æ£’çš„å‘é‡ç‰¹å¾è®¡ç®—
        vector_features = self._compute_robust_vector_features(pos, edge_index, is_covalent)
        
        # 9. æ„å»ºæœ€ç»ˆçš„ Data å¯¹è±¡
        data = Data(
            x=z,
            pos=pos,
            edge_index=edge_index,
            edge_attr=edge_attr,
            vector_features=vector_features,
            mask_interface=mask_interface,
            is_ligand=is_ligand,
            residue_index=residue_index,
            num_nodes=pos.size(0)
        )
        
        # æ·»åŠ å¢å¼ºçš„å…ƒæ•°æ®
        data.pdb_id = meta['pdb_id']
        data.chains = meta['chains']
        data.is_patched = is_patched
        data.patch_index = patch_index
        data.original_num_nodes = original_num_nodes
        
        return data
