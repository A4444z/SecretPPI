
"""
GlueVAE ä¸»æ¨¡å‹æ¶æ„ã€‚
å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢ç”Ÿæˆã€‚

æ¶æ„æ¦‚è¿°ï¼š
1. ç¼–ç å™¨ï¼šå¤šå±‚ PaiNNï¼Œæå–å…¨åŸå­ç‰¹å¾
2. ç“¶é¢ˆå±‚ï¼šåŸå­ -&gt; æ®‹åŸº Poolingï¼Œé™ç»´åˆ°æ®‹åŸºçº§åˆ«
3. æ½œåœ¨ç©ºé—´ï¼šé‡å‚æ•°åŒ–é‡‡æ ·
4. è§£ç å™¨ï¼šæ¡ä»¶ç”Ÿæˆï¼Œæ®‹åŸº -&gt; åŸå­ super-resolution
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_mean, scatter_sum

from src.models.layers_solo import PaiNNEncoder
from src.utils.loss_solo import CoordinateDecoder

# ================= ğŸš¨ æ–°å¢ RBF ç±» =================
class GaussianSmearing(nn.Module):
    """
    å¾„å‘åŸºå‡½æ•° (RBF) å±•å¼€ï¼Œç”¨äºå°†æ ‡é‡è·ç¦»æ˜ å°„ä¸ºé«˜ç»´å‘é‡ã€‚
    """
    def __init__(self, start=0.0, stop=10.0, num_gaussians=16):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        # è®¡ç®—é«˜æ–¯å‡½æ•°çš„å®½åº¦ç³»æ•°
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer('offset', offset)

    def forward(self, dist):
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ===================================================

class ResiduePooling(nn.Module):
    """
    åŸå­åˆ°æ®‹åŸºçš„ Pooling å±‚ã€‚
    ä½¿ç”¨ scatter_mean å°†åŒä¸€æ®‹åŸºçš„åŸå­ç‰¹å¾èšåˆä¸ºæ®‹åŸºç‰¹å¾ã€‚
    """
    
    def __init__(self, reduce='mean'):
        super().__init__()
        self.reduce = reduce
        
    def forward(
        self,
        atom_features,
        residue_index
    ):
        """
        å‚æ•°:
            atom_features: [N, hidden_dim] åŸå­çº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾ï¼ŒR ä¸ºæ®‹åŸºæ•°
        """
        if self.reduce == 'mean':
            return scatter_mean(atom_features, residue_index, dim=0)
        elif self.reduce == 'sum':
            return scatter_sum(atom_features, residue_index, dim=0)
        else:
            raise ValueError(f"Unknown reduce type: {self.reduce}")


class ResidueToAtomUnpooling(nn.Module):
    """
    æ®‹åŸºåˆ°åŸå­çš„ Unpooling å±‚ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾å¹¿æ’­å›åŸå­çº§åˆ«ã€‚
    """
    
    def forward(
        self,
        residue_features,
        residue_index
    ):
        """
        å‚æ•°:
            residue_features: [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, hidden_dim] åŸå­çº§ç‰¹å¾
        """
        return residue_features[residue_index]


class LatentEncoder(nn.Module):
    """
    æ½œåœ¨ç©ºé—´ç¼–ç å™¨ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾æ˜ å°„åˆ°æ½œåœ¨åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚
    """
    
    def __init__(self, hidden_dim, latent_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
        )
        self.mu_head = nn.Linear(hidden_dim, latent_dim)
        self.logvar_head = nn.Linear(hidden_dim, latent_dim)
        
    def forward(self, x):
        """
        å‚æ•°:
            x: [R, hidden_dim] æ®‹åŸºç‰¹å¾
        è¿”å›:
            (mu, logvar): æ¯ä¸ª [R, latent_dim]
        """
        h = self.mlp(x)
        mu = self.mu_head(h)
        logvar = self.logvar_head(h)
        return mu, logvar


class LatentDecoder(nn.Module):
    """
    æ½œåœ¨ç©ºé—´è§£ç å™¨ã€‚
    å°†æ½œåœ¨å‘é‡æ˜ å°„å›æ®‹åŸºçº§ç‰¹å¾ã€‚
    """
    
    def __init__(self, latent_dim, hidden_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
    def forward(self, z):
        """
        å‚æ•°:
            z: [R, latent_dim] æ½œåœ¨å‘é‡
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºç‰¹å¾
        """
        return self.mlp(z)


class ConditionalPaiNNDecoder(nn.Module):
    """
    æ¡ä»¶ PaiNN è§£ç å™¨ã€‚
    ç»“åˆå—ä½“åŸå­å’Œæ½œåœ¨ä¿¡æ¯ï¼Œç”Ÿæˆé…ä½“åŸå­åæ ‡ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        num_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # PaiNN ç¼–ç å™¨ä½œä¸ºè§£ç å™¨ä¸»å¹²
        self.painn = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åæ ‡é¢„æµ‹å¤´
        #self.coord_decoder = CoordinateDecoder(hidden_dim, num_layers=2)
        self.v_proj = nn.Linear(hidden_dim, 1, bias=False)
        
    def forward(
        self,
        atom_latent,
        z_atom,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        å‚æ•°:
            atom_latent: [N, hidden_dim] åŸå­çº§æ½œåœ¨ç‰¹å¾
            z_atom: [N] åŸå­åºæ•°
            vector_features: [N, 3] å‘é‡ç‰¹å¾
            edge_index: [2, E] è¾¹ç´¢å¼•
            edge_attr: [E, edge_dim] è¾¹ç‰¹å¾
            pos: [N, 3] åˆå§‹åæ ‡ï¼ˆå—ä½“å›ºå®šï¼Œé…ä½“å¯è°ƒæ•´ï¼‰
            residue_index: [N] æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»/æ›´æ–°
        """
        # åˆå§‹åŒ–æ ‡é‡ç‰¹å¾ï¼šåµŒå…¥ + æ½œåœ¨ç‰¹å¾
        s_initial = self.painn.embedding(z_atom) + atom_latent
        
        # é€šè¿‡ PaiNN æå–ç‰¹å¾
        s, v = self.painn(z_atom, vector_features, edge_index, edge_attr, pos, initial_s=s_initial)
        
        # é¢„æµ‹åæ ‡åç§»
        #delta_pos = self.coord_decoder(s)
        # å®ƒçš„ä½œç”¨æ˜¯æŠŠ [N, 3, hidden_dim] çš„å‘é‡ç‰¹å¾å‹ç¼©æˆ [N, 3, 1] çš„ç‰©ç†ä½ç§»
        #self.v_proj = nn.Linear(hidden_dim, 1, bias=False)

        # âœ… å®Œç¾ä¿®å¤ï¼šå°† [N, 128, 3] è½¬ç½®ä¸º [N, 3, 128]
        # è¿™æ ·çº¿æ€§å±‚å°±ä¼šå¯¹ 128 è¿›è¡Œè®¡ç®—ï¼Œè¾“å‡º [N, 3, 1]
        # æœ€å squeeze(-1) æŒ¤æ‰æœ€åé‚£ä¸ª 1ï¼Œç•™ä¸‹å®Œç¾çš„ [N, 3] åæ ‡åç§»ï¼
        delta_pos = self.v_proj(v.transpose(1, 2)).squeeze(-1)
        
        return delta_pos


class GlueVAE(nn.Module):
    """
    GlueVAE ä¸»æ¨¡å‹ã€‚
    å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        latent_dim=32,
        num_encoder_layers=6,
        num_decoder_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨
        self.encoder = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_encoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åŸå­ -&gt; æ®‹åŸº Pooling
        self.residue_pooling = ResiduePooling(reduce='mean')
        
        # æ½œåœ¨ç©ºé—´
        self.latent_encoder = LatentEncoder(hidden_dim, latent_dim)
        self.latent_decoder = LatentDecoder(latent_dim, hidden_dim)
        
        # æ®‹åŸº -&gt; åŸå­ Unpooling
        self.residue_unpooling = ResidueToAtomUnpooling()
        
        # è§£ç å™¨
        self.decoder = ConditionalPaiNNDecoder(
            hidden_dim=hidden_dim,
            num_layers=num_decoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )

        # ================= ğŸš¨ æ–°å¢ RBF å±‚ =================
        # edge_dim (19) - æ‹“æ‰‘ç‰¹å¾ (3) = 16 ç»´çš„é«˜æ–¯ç‰¹å¾
        self.rbf = GaussianSmearing(
            start=0.0, 
            stop=10.0, 
            num_gaussians=edge_dim - 3
        )
        # ==================================================
        
    def reparameterize(self, mu, logvar):
        """
        é‡å‚æ•°åŒ–æŠ€å·§ã€‚
        z = mu + sigma * epsilon, epsilon ~ N(0, 1)
        """
        if self.training:
            # ====== [æ–°å¢] é˜²æº¢å‡º clamp ======
            logvar = torch.clamp(logvar, min=-20.0, max=20.0)
            # ================================
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu
            
    def encode(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        ç¼–ç è¿‡ç¨‹ï¼šè¾“å…¥ -&gt; æ½œåœ¨åˆ†å¸ƒå‚æ•°ã€‚
        """
                # PaiNN ç¼–ç å™¨
        s, v = self.encoder(z, vector_features, edge_index, edge_attr, pos)

        # åŸå­ -> æ®‹åŸº Pooling
        res_features = self.residue_pooling(s, residue_index)

        # ===== [DEBUG] ä¸€æ¬¡æ€§æ‰“å° =====
        if not hasattr(self, "_debug_once_encode"):
            self._debug_once_encode = False
        if not self._debug_once_encode:
            print("\n[DEBUG][GlueVAE.encode]")
            print("  s finite:", torch.isfinite(s).all().item())
            print("  residue_index min/max:", int(residue_index.min()), int(residue_index.max()))
            print("  residue unique:", int(residue_index.unique().numel()),
                " / max+1:", int(residue_index.max().item()) + 1)
            print("  res_features finite:", torch.isfinite(res_features).all().item(),
                "shape:", tuple(res_features.shape))
        # ============================

        # æ½œåœ¨åˆ†å¸ƒ
        mu, logvar = self.latent_encoder(res_features)

        if not self._debug_once_encode:
            print("  mu finite:", torch.isfinite(mu).all().item())
            print("  logvar finite:", torch.isfinite(logvar).all().item())
            if torch.isfinite(logvar).all():
                print("  logvar range:", float(logvar.min()), float(logvar.max()))
            self._debug_once_encode = True

        return mu, logvar

        
    def decode(
        self,
        z_latent,
        z_atom,
        fake_vector_features, # ğŸ‘ˆ æ¥æ”¶å‡çš„å‘é‡
        edge_index,
        fake_edge_attr,       # ğŸ‘ˆ æ¥æ”¶å‡çš„è·ç¦»
        fake_pos,             # ğŸ‘ˆ æ¥æ”¶å‡çš„èµ·ç‚¹åæ ‡
        residue_index
    ):
        """
        è§£ç è¿‡ç¨‹ï¼šæ½œåœ¨å‘é‡ -> åæ ‡ã€‚
        """
        # æ½œåœ¨ -> æ®‹åŸºç‰¹å¾
        res_features = self.latent_decoder(z_latent)
        
        # Unpoolingï¼šæ®‹åŸºç‰¹å¾ -> åŸå­ç‰¹å¾
        atom_latent = self.residue_unpooling(res_features, residue_index)
        
        # é€šè¿‡è§£ç å™¨ (æ­¤æ—¶ Decoder åªèƒ½çœ‹åˆ°ççŒœçš„ fake_pos å’Œ fake ç‰¹å¾)
        delta_pos = self.decoder(
            atom_latent, z_atom, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos, residue_index
        )
        
        # ğŸš¨ ç»ˆæä¿®å¤ï¼šå¿…é¡»æ˜¯åœ¨ fake_pos çš„åŸºç¡€ä¸Šè¿›è¡Œåç§»ï¼ç»ä¸èƒ½åŠ ä¸ŠçœŸå®çš„ posï¼
        pos_pred = fake_pos + delta_pos
        
        return pos_pred

    def forward(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        mask_interface=None,  # ğŸ‘ˆ æ–°å¢
        batch_idx=None        # ğŸ‘ˆ æ–°å¢ï¼šå¿…é¡»æœ‰è¿™ä¸ªæ‰èƒ½åŒºåˆ†ä¸åŒçš„å¤åˆç‰©
    ):
        mu, logvar = self.encode(
            z, vector_features, edge_index, edge_attr, pos, residue_index
        )
        z_latent = self.reparameterize(mu, logvar)
        
        noise_scale = 4.0 
        fake_pos = pos + torch.randn_like(pos) * noise_scale
        
        # ================= ğŸš¨ ç»ˆææ€æ‹›ï¼šPyG Batched Interface Block Masking =================
        if self.training and mask_interface is not None and batch_idx is not None:
            # åˆ›å»ºä¸€ä¸ªå…¨å›¾çš„ç©º Mask
            block_mask = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)
            
            # è·å– Batch ä¸­æ€»å…±æœ‰å¤šå°‘ä¸ªç‹¬ç«‹çš„å›¾ (æ¯”å¦‚ 16 ä¸ª)
            num_graphs = int(batch_idx.max().item()) + 1
            
            # å¯¹æ¯ä¸€ä¸ªå›¾æ‰§è¡Œç‹¬ç«‹çš„ç•Œé¢è½°ç‚¸
            for i in range(num_graphs):
                # æ‰¾åˆ°å±äºç¬¬ i ä¸ªå›¾çš„æ‰€æœ‰åŸå­çš„å…¨å±€ç´¢å¼•
                graph_node_idx = torch.nonzero(batch_idx == i).squeeze(-1)
                
                # æå–è¿™ä¸ªå›¾çš„ç•Œé¢æ©ç 
                graph_interface_mask = mask_interface[graph_node_idx]
                graph_interface_nodes = graph_node_idx[torch.nonzero(graph_interface_mask).squeeze(-1)]
                
                # å¦‚æœè¿™ä¸ªå›¾æœ‰ç•Œé¢åŸå­
                if graph_interface_nodes.numel() > 0:
                    # 1. éšæœºé€‰ä¸€ä¸ªçˆ†ç‚¸ä¸­å¿ƒ
                    center_idx = graph_interface_nodes[torch.randint(0, graph_interface_nodes.numel(), (1,))]
                    center_pos = pos[center_idx]
                    
                    # 2. ç®—è¿™ä¸ªå›¾é‡Œæ‰€æœ‰åŸå­åˆ°ä¸­å¿ƒçš„è·ç¦»
                    dist_to_center = torch.norm(pos[graph_node_idx] - center_pos, p=2, dim=-1)
                    
                    # 3. æ‰¾å‡ºå±€éƒ¨ 10 åŸƒå†…çš„åŸå­
                    local_block_mask = dist_to_center < 10.0
                    
                    # 4. æŠŠè¢«ç‚¸çš„åŸå­æ˜ å°„å›å…¨å±€çš„ block_mask é‡Œ
                    global_block_mask_idx = graph_node_idx[local_block_mask]
                    block_mask[global_block_mask_idx] = True
            
            # ç»Ÿè®¡æ€»å…±è¢«æ©ç çš„åŸå­
            num_masked = block_mask.sum()
            if num_masked > 0:
                # å¡Œé™·åˆ°å„è‡ªåŸå­çš„è´¨å¿ƒï¼ˆè¿™é‡Œåšäº†ç®€åŒ–å¤„ç†ï¼Œå¡Œé™·åˆ°åŸç‚¹é™„è¿‘å¹¶æ–½åŠ æ‰°åŠ¨ï¼Œå½»åº•ç ´åå…¶ç©ºé—´ç»“æ„ï¼‰
                independent_noise = torch.randn((num_masked, 3), device=pos.device) * 0.1
                fake_pos[block_mask] = independent_noise
        # =========================================================================
            
        edge_type = edge_attr[:, :3]
        
        # é‡æ–°è®¡ç®—è·ç¦» (é˜²å´©æºƒçš„ Safe Norm)
        row, col = edge_index
        fake_diff = fake_pos[row] - fake_pos[col]
        dist_sq = (fake_diff ** 2).sum(dim=-1)
        fake_dist = torch.sqrt(dist_sq + 1e-8) 
        
        fake_rbf_feat = self.rbf(fake_dist)
        fake_edge_attr = torch.cat([edge_type, fake_rbf_feat], dim=-1)
        fake_vector_features = torch.zeros_like(vector_features)

        pos_pred = self.decode(
            z_latent, z, fake_vector_features, 
            edge_index, fake_edge_attr, fake_pos, residue_index
        )
        
        return pos_pred, mu, logvar
        
    @torch.no_grad()
    def sample(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        num_samples=1,
        mask_interface=None,  # ğŸ‘ˆ æ–°å¢
        batch_idx=None        # ğŸ‘ˆ æ–°å¢ï¼šå¿…é¡»æœ‰è¿™ä¸ªæ‰èƒ½åŒºåˆ†ä¸åŒçš„å¤åˆç‰©
    ):
        """
        ä»æ½œåœ¨ç©ºé—´é‡‡æ ·ç”Ÿæˆå¤šä¸ªæ ·æœ¬ã€‚
        """
        mu, logvar = self.encode(
            z, vector_features, edge_index, edge_attr, pos, residue_index
        )
        
        samples = []
        for _ in range(num_samples):
            z_latent = self.reparameterize(mu, logvar)
            
            # ================= ğŸš¨ ä¿®å¤ sample æ–¹æ³•çš„æ•°æ®æ³„éœ²ä¸ç»´åº¦ =================
            # ğŸš¨ æ‹¯æ•‘å›¾ç¥ç»ç½‘ç»œçš„å‘½è„‰ï¼šåœ¨çœŸå®åæ ‡ä¸Šæ–½åŠ å°å¹…åº¦æ‰°åŠ¨ï¼Œè€Œä¸æ˜¯å®Œå…¨æŠ¹æ€
            noise_scale = 4.0 
            fake_pos = pos + torch.randn_like(pos) * noise_scale  
            edge_type = edge_attr[:, :3]            
            
            row, col = edge_index
            fake_diff = fake_pos[row] - fake_pos[col]
            fake_dist = torch.norm(fake_diff, p=2, dim=-1) + 1e-6 
            
            fake_rbf_feat = self.rbf(fake_dist)
            fake_edge_attr = torch.cat([edge_type, fake_rbf_feat], dim=-1)
            
            # èŠ‚ç‚¹çº§åˆ«åˆå§‹å‘é‡ï¼ŒåŒæ ·ç”¨å…¨é›¶
            fake_vector_features = torch.zeros_like(vector_features)
            # ===============================================================
            # ä½¿ç”¨é‡ç®—åçš„ fake ç‰¹å¾è¿›è¡Œè§£ç 
            pos_pred = self.decode(
                z_latent, z, fake_vector_features,
                edge_index, fake_edge_attr, fake_pos, residue_index
            )
            samples.append(pos_pred)
            
        return torch.stack(samples, dim=0)
