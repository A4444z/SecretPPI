
"""
GlueVAE ä¸»æ¨¡å‹æ¶æ„ã€‚
å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢ç”Ÿæˆã€‚

æ¶æ„æ¦‚è¿°ï¼š
1. ç¼–ç å™¨ï¼šå¤šå±‚ PaiNNï¼Œæå–å…¨åŸå­ç‰¹å¾
2. ç“¶é¢ˆå±‚ï¼šåŸå­ -&gt; æ®‹åŸº Poolingï¼Œé™ç»´åˆ°æ®‹åŸºçº§åˆ«
3. æ½œåœ¨ç©ºé—´ï¼šé‡å‚æ•°åŒ–é‡‡æ ·
4. è§£ç å™¨ï¼šæ¡ä»¶ç”Ÿæˆï¼Œæ®‹åŸº -&gt; åŸå­ super-resolution
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_mean, scatter_sum

from src.models.layers_solo import PaiNNEncoder
from src.utils.loss_solo import CoordinateDecoder


class ResiduePooling(nn.Module):
    """
    åŸå­åˆ°æ®‹åŸºçš„ Pooling å±‚ã€‚
    ä½¿ç”¨ scatter_mean å°†åŒä¸€æ®‹åŸºçš„åŸå­ç‰¹å¾èšåˆä¸ºæ®‹åŸºç‰¹å¾ã€‚
    """
    
    def __init__(self, reduce='mean'):
        super().__init__()
        self.reduce = reduce
        
    def forward(
        self,
        atom_features,
        residue_index
    ):
        """
        å‚æ•°:
            atom_features: [N, hidden_dim] åŸå­çº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾ï¼ŒR ä¸ºæ®‹åŸºæ•°
        """
        if self.reduce == 'mean':
            return scatter_mean(atom_features, residue_index, dim=0)
        elif self.reduce == 'sum':
            return scatter_sum(atom_features, residue_index, dim=0)
        else:
            raise ValueError(f"Unknown reduce type: {self.reduce}")


class ResidueToAtomUnpooling(nn.Module):
    """
    æ®‹åŸºåˆ°åŸå­çš„ Unpooling å±‚ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾å¹¿æ’­å›åŸå­çº§åˆ«ã€‚
    """
    
    def forward(
        self,
        residue_features,
        residue_index
    ):
        """
        å‚æ•°:
            residue_features: [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, hidden_dim] åŸå­çº§ç‰¹å¾
        """
        return residue_features[residue_index]


class LatentEncoder(nn.Module):
    """
    æ½œåœ¨ç©ºé—´ç¼–ç å™¨ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾æ˜ å°„åˆ°æ½œåœ¨åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚
    """
    
    def __init__(self, hidden_dim, latent_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
        )
        self.mu_head = nn.Linear(hidden_dim, latent_dim)
        self.logvar_head = nn.Linear(hidden_dim, latent_dim)
        
    def forward(self, x):
        """
        å‚æ•°:
            x: [R, hidden_dim] æ®‹åŸºç‰¹å¾
        è¿”å›:
            (mu, logvar): æ¯ä¸ª [R, latent_dim]
        """
        h = self.mlp(x)
        mu = self.mu_head(h)
        logvar = self.logvar_head(h)
        return mu, logvar


class LatentDecoder(nn.Module):
    """
    æ½œåœ¨ç©ºé—´è§£ç å™¨ã€‚
    å°†æ½œåœ¨å‘é‡æ˜ å°„å›æ®‹åŸºçº§ç‰¹å¾ã€‚
    """
    
    def __init__(self, latent_dim, hidden_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
    def forward(self, z):
        """
        å‚æ•°:
            z: [R, latent_dim] æ½œåœ¨å‘é‡
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºç‰¹å¾
        """
        return self.mlp(z)


class ConditionalPaiNNDecoder(nn.Module):
    """
    æ¡ä»¶ PaiNN è§£ç å™¨ã€‚
    ç»“åˆå—ä½“åŸå­å’Œæ½œåœ¨ä¿¡æ¯ï¼Œç”Ÿæˆé…ä½“åŸå­åæ ‡ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        num_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # PaiNN ç¼–ç å™¨ä½œä¸ºè§£ç å™¨ä¸»å¹²
        self.painn = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åæ ‡é¢„æµ‹å¤´
        self.coord_decoder = CoordinateDecoder(hidden_dim, num_layers=2)
        
    def forward(
        self,
        atom_latent,
        z_atom,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        å‚æ•°:
            atom_latent: [N, hidden_dim] åŸå­çº§æ½œåœ¨ç‰¹å¾
            z_atom: [N] åŸå­åºæ•°
            vector_features: [N, 3] å‘é‡ç‰¹å¾
            edge_index: [2, E] è¾¹ç´¢å¼•
            edge_attr: [E, edge_dim] è¾¹ç‰¹å¾
            pos: [N, 3] åˆå§‹åæ ‡ï¼ˆå—ä½“å›ºå®šï¼Œé…ä½“å¯è°ƒæ•´ï¼‰
            residue_index: [N] æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»/æ›´æ–°
        """
        # åˆå§‹åŒ–æ ‡é‡ç‰¹å¾ï¼šåµŒå…¥ + æ½œåœ¨ç‰¹å¾
        s_initial = self.painn.embedding(z_atom) + atom_latent
        
        # é€šè¿‡ PaiNN æå–ç‰¹å¾
        s, v = self.painn(z_atom, vector_features, edge_index, edge_attr, pos, initial_s=s_initial)
        
        # é¢„æµ‹åæ ‡åç§»
        delta_pos = self.coord_decoder(s)
        
        return delta_pos


class GlueVAE(nn.Module):
    """
    GlueVAE ä¸»æ¨¡å‹ã€‚
    å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        latent_dim=32,
        num_encoder_layers=6,
        num_decoder_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨
        self.encoder = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_encoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åŸå­ -&gt; æ®‹åŸº Pooling
        self.residue_pooling = ResiduePooling(reduce='mean')
        
        # æ½œåœ¨ç©ºé—´
        self.latent_encoder = LatentEncoder(hidden_dim, latent_dim)
        self.latent_decoder = LatentDecoder(latent_dim, hidden_dim)
        
        # æ®‹åŸº -&gt; åŸå­ Unpooling
        self.residue_unpooling = ResidueToAtomUnpooling()
        
        # è§£ç å™¨
        self.decoder = ConditionalPaiNNDecoder(
            hidden_dim=hidden_dim,
            num_layers=num_decoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
    def reparameterize(self, mu, logvar):
        """
        é‡å‚æ•°åŒ–æŠ€å·§ã€‚
        z = mu + sigma * epsilon, epsilon ~ N(0, 1)
        """
        if self.training:
            # ====== [æ–°å¢] é˜²æº¢å‡º clamp ======
            logvar = torch.clamp(logvar, min=-20.0, max=20.0)
            # ================================
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu
            
    def encode(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        ç¼–ç è¿‡ç¨‹ï¼šè¾“å…¥ -&gt; æ½œåœ¨åˆ†å¸ƒå‚æ•°ã€‚
        """
                # PaiNN ç¼–ç å™¨
        s, v = self.encoder(z, vector_features, edge_index, edge_attr, pos)

        # åŸå­ -> æ®‹åŸº Pooling
        res_features = self.residue_pooling(s, residue_index)

        # ===== [DEBUG] ä¸€æ¬¡æ€§æ‰“å° =====
        if not hasattr(self, "_debug_once_encode"):
            self._debug_once_encode = False
        if not self._debug_once_encode:
            print("\n[DEBUG][GlueVAE.encode]")
            print("  s finite:", torch.isfinite(s).all().item())
            print("  residue_index min/max:", int(residue_index.min()), int(residue_index.max()))
            print("  residue unique:", int(residue_index.unique().numel()),
                " / max+1:", int(residue_index.max().item()) + 1)
            print("  res_features finite:", torch.isfinite(res_features).all().item(),
                "shape:", tuple(res_features.shape))
        # ============================

        # æ½œåœ¨åˆ†å¸ƒ
        mu, logvar = self.latent_encoder(res_features)

        if not self._debug_once_encode:
            print("  mu finite:", torch.isfinite(mu).all().item())
            print("  logvar finite:", torch.isfinite(logvar).all().item())
            if torch.isfinite(logvar).all():
                print("  logvar range:", float(logvar.min()), float(logvar.max()))
            self._debug_once_encode = True

        return mu, logvar

        
    def decode(
        self,
        z_latent,
        z_atom,
        fake_vector_features, # ğŸ‘ˆ æ¥æ”¶å‡çš„å‘é‡
        edge_index,
        fake_edge_attr,       # ğŸ‘ˆ æ¥æ”¶å‡çš„è·ç¦»
        fake_pos,             # ğŸ‘ˆ æ¥æ”¶å‡çš„èµ·ç‚¹åæ ‡
        residue_index
    ):
        """
        è§£ç è¿‡ç¨‹ï¼šæ½œåœ¨å‘é‡ -> åæ ‡ã€‚
        """
        # æ½œåœ¨ -> æ®‹åŸºç‰¹å¾
        res_features = self.latent_decoder(z_latent)
        
        # Unpoolingï¼šæ®‹åŸºç‰¹å¾ -> åŸå­ç‰¹å¾
        atom_latent = self.residue_unpooling(res_features, residue_index)
        
        # é€šè¿‡è§£ç å™¨ (æ­¤æ—¶ Decoder åªèƒ½çœ‹åˆ°ççŒœçš„ fake_pos å’Œ fake ç‰¹å¾)
        delta_pos = self.decoder(
            atom_latent, z_atom, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos, residue_index
        )
        
        # ğŸš¨ ç»ˆæä¿®å¤ï¼šå¿…é¡»æ˜¯åœ¨ fake_pos çš„åŸºç¡€ä¸Šè¿›è¡Œåç§»ï¼ç»ä¸èƒ½åŠ ä¸ŠçœŸå®çš„ posï¼
        pos_pred = fake_pos + delta_pos
        
        return pos_pred

    def forward(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        # 1. ç¼–ç  (è¿™é‡Œæ²¡æœ‰æ³„éœ²ï¼ŒEncoder éœ€è¦çœ‹çœŸå®æ•°æ®æå–ä¿¡æ¯)
        mu, logvar = self.encode(
            z, vector_features, edge_index, edge_attr, pos, residue_index
        )
        z_latent = self.reparameterize(mu, logvar)
        
        # ================= ğŸš¨ æ–©æ–­æ³„éœ²ï¼šåˆ›å»ºç”Ÿæˆèµ·ç‚¹ =================
        # æˆ‘ä»¬ç»™ Decoder ä¸€ä¸ªå®Œå…¨ççŒœçš„èµ·ç‚¹ï¼Œæ¯”å¦‚åŸç‚¹é™„è¿‘çš„éšæœºé«˜æ–¯å™ªå£°
        # è¿™æ ·å®ƒå°±ä¸§å¤±äº†çœŸå®åæ ‡çš„ä¿¡æ¯
        fake_pos = torch.randn_like(pos) * 5.0  # ä¹˜ä»¥ 5.0 åŸƒæ”¾å¤§å™ªå£°ï¼Œæ¨¡æ‹ŸæœªæŠ˜å çŠ¶æ€
        
        # âš ï¸ å…³é”®éš¾ç‚¹ï¼šæ—¢ç„¶åæ ‡å˜äº†ï¼ŒPaiNN ä¾èµ–çš„è·ç¦»(edge_attr)å’Œæ–¹å‘(vector_features)ä¹Ÿå¿…é¡»é‡ç®—ï¼
        # å¦åˆ™å¦‚æœä½ æŠŠ fake_pos åŠ ä¸ŠçœŸå®çš„ edge_attr ä¼ è¿›å»ï¼Œä¾ç„¶ä¼šæ³„éœ²çœŸå®çš„è·ç¦»ç­”æ¡ˆï¼
        
        row, col = edge_index
        fake_diff = fake_pos[row] - fake_pos[col]
        fake_dist = torch.norm(fake_diff, p=2, dim=-1)
        
        # --- [è¿™é‡Œéœ€è¦ä½ è¡¥å……ä½ çš„ RBF å’Œç‰¹å¾è®¡ç®—ä»£ç ] ---
        # ä½ å¿…é¡»æŠŠ dataset.py é‡Œè®¡ç®— rbf å’Œ vector_features çš„é€»è¾‘æ¬åˆ°è¿™é‡Œï¼
        # ä¼ªä»£ç ç¤ºä¾‹ï¼š
        # fake_rbf_feat = self.rbf(fake_dist)
        # fake_edge_attr = torch.cat([edge_type, fake_rbf_feat], dim=-1) # edge_type å¯ä»¥ä¿ç•™çœŸå®çš„(å¦‚æ˜¯å¦å…±ä»·é”®)
        # fake_vector_features = fake_diff / (fake_dist.unsqueeze(-1) + 1e-6)
        # ----------------------------------------------
        
        # ä¸ºäº†è®©ä½ èƒ½â€œç«‹åˆ»è·‘é€šå¹¶çœ‹åˆ° Loss æ¢å¤æ­£å¸¸â€ï¼Œå¦‚æœä½ è¿˜æ²¡å†™å¥½é‡ç®—ç‰¹å¾çš„å‡½æ•°ï¼Œ
        # å¯ä»¥å…ˆç”¨æç«¯çš„æš´åŠ›åˆ‡æ–­æ³•ï¼ˆä¸æ¨èé•¿æœŸä½¿ç”¨ï¼Œä½†èƒ½æ‰“ç ´ 0 çš„åƒµå±€ï¼‰ï¼š
        fake_vector_features = torch.zeros_like(vector_features)
        fake_edge_attr = torch.zeros_like(edge_attr)
        # ==============================================================

        # è§£ç ï¼šå¼ºè¿« Decoder åœ¨â€œä¸€æ— æ‰€çŸ¥â€çš„æ¶åŠ£ç¯å¢ƒä¸‹ï¼Œä»…é  z_latent è¿˜åŸ 3D ç»“æ„
        pos_pred = self.decode(
            z_latent, z, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos, residue_index
        )
        
        return pos_pred, mu, logvar
        
    @torch.no_grad()
    def sample(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        num_samples=1
    ):
        """
        ä»æ½œåœ¨ç©ºé—´é‡‡æ ·ç”Ÿæˆå¤šä¸ªæ ·æœ¬ã€‚
        """
        mu, logvar = self.encode(
            z, vector_features, edge_index, edge_attr, pos, residue_index
        )
        
        samples = []
        for _ in range(num_samples):
            z_latent = self.reparameterize(mu, logvar)
            pos_pred = self.decode(
                z_latent, z, vector_features,
                edge_index, edge_attr, pos, residue_index
            )
            samples.append(pos_pred)
            
        return torch.stack(samples, dim=0)

