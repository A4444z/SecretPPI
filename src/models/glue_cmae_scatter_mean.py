
"""
Geminiä¸»åˆ€ä¿®æ”¹çš„cmaeæ¶æ„ï¼Œfrom glue_vae_solo.py
GlueVAE ä¸»æ¨¡å‹æ¶æ„ã€‚
å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢ç”Ÿæˆã€‚

æ¶æ„æ¦‚è¿°ï¼š
1. ç¼–ç å™¨ï¼šå¤šå±‚ PaiNNï¼Œæå–å…¨åŸå­ç‰¹å¾
2. ç“¶é¢ˆå±‚ï¼šåŸå­ -&gt; æ®‹åŸº Poolingï¼Œé™ç»´åˆ°æ®‹åŸºçº§åˆ«
3. æ½œåœ¨ç©ºé—´ï¼šé‡å‚æ•°åŒ–é‡‡æ ·
4. è§£ç å™¨ï¼šæ¡ä»¶ç”Ÿæˆï¼Œæ®‹åŸº -&gt; åŸå­ super-resolution
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_mean, scatter_sum

from src.models.layers_solo import PaiNNEncoder
from src.utils.loss_solo import CoordinateDecoder

# ================= ğŸš¨ æ–°å¢ RBF ç±» =================
class GaussianSmearing(nn.Module):
    """
    å¾„å‘åŸºå‡½æ•° (RBF) å±•å¼€ï¼Œç”¨äºå°†æ ‡é‡è·ç¦»æ˜ å°„ä¸ºé«˜ç»´å‘é‡ã€‚
    """
    def __init__(self, start=0.0, stop=10.0, num_gaussians=16):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        # è®¡ç®—é«˜æ–¯å‡½æ•°çš„å®½åº¦ç³»æ•°
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer('offset', offset)

    def forward(self, dist):
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ===================================================

class ResiduePooling(nn.Module):
    """
    åŸå­åˆ°æ®‹åŸºçš„ Pooling å±‚ã€‚
    ä½¿ç”¨ scatter_mean å°†åŒä¸€æ®‹åŸºçš„åŸå­ç‰¹å¾èšåˆä¸ºæ®‹åŸºç‰¹å¾ã€‚
    """
    
    def __init__(self, reduce='mean'):
        super().__init__()
        self.reduce = reduce
        
    def forward(
        self,
        atom_features,
        residue_index
    ):
        """
        å‚æ•°:
            atom_features: [N, hidden_dim] åŸå­çº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾ï¼ŒR ä¸ºæ®‹åŸºæ•°
        """
        if self.reduce == 'mean':
            return scatter_mean(atom_features, residue_index, dim=0)
        elif self.reduce == 'sum':
            return scatter_sum(atom_features, residue_index, dim=0)
        else:
            raise ValueError(f"Unknown reduce type: {self.reduce}")


class ResidueToAtomUnpooling(nn.Module):
    """
    æ®‹åŸºåˆ°åŸå­çš„ Unpooling å±‚ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾å¹¿æ’­å›åŸå­çº§åˆ«ã€‚
    """
    
    def forward(
        self,
        residue_features,
        residue_index
    ):
        """
        å‚æ•°:
            residue_features: [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, hidden_dim] åŸå­çº§ç‰¹å¾
        """
        return residue_features[residue_index]


class Projector(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ (Projection Head)ã€‚
    3å±‚ MLPï¼Œæœ€åè¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œå°†ç‰¹å¾æ˜ å°„åˆ°å¯¹æ¯”ç©ºé—´ã€‚
    """
    def __init__(self, hidden_dim, proj_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, proj_dim)
        )

    def forward(self, x):
        """
        å‚æ•°:
            x: [R, hidden_dim] æ®‹åŸºç‰¹å¾
        è¿”å›:
            z: [R, proj_dim] L2å½’ä¸€åŒ–åçš„å¯¹æ¯”å‘é‡
        """
        z = self.mlp(x)
        # ğŸš¨ æå…¶å…³é”®çš„ä¸€æ­¥ï¼šå¯¹ç‰¹å¾è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œä½¿å…¶åˆ†å¸ƒåœ¨è¶…çƒé¢ä¸Š
        z = F.normalize(z, p=2, dim=-1)
        return z


class ConditionalPaiNNDecoder(nn.Module):
    """
    æ¡ä»¶ PaiNN è§£ç å™¨ã€‚
    ç»“åˆå—ä½“åŸå­å’Œæ½œåœ¨ä¿¡æ¯ï¼Œç”Ÿæˆé…ä½“åŸå­åæ ‡ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        num_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # PaiNN ç¼–ç å™¨ä½œä¸ºè§£ç å™¨ä¸»å¹²
        self.painn = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åæ ‡é¢„æµ‹å¤´
        #self.coord_decoder = CoordinateDecoder(hidden_dim, num_layers=2)
        self.v_proj = nn.Linear(hidden_dim, 1, bias=False)
        
    def forward(
        self,
        atom_latent,
        z_atom,
        vector_features,
        edge_index,
        edge_attr,
        pos
    ):
        """
        å‚æ•°:
            atom_latent: [N, hidden_dim] åŸå­çº§æ½œåœ¨ç‰¹å¾
            z_atom: [N] åŸå­åºæ•°
            vector_features: [N, 3] å‘é‡ç‰¹å¾
            edge_index: [2, E] è¾¹ç´¢å¼•
            edge_attr: [E, edge_dim] è¾¹ç‰¹å¾
            pos: [N, 3] åˆå§‹åæ ‡ï¼ˆå—ä½“å›ºå®šï¼Œé…ä½“å¯è°ƒæ•´ï¼‰
            residue_index: [N] æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»/æ›´æ–°
        """
        # åˆå§‹åŒ–æ ‡é‡ç‰¹å¾ï¼šåµŒå…¥ + æ½œåœ¨ç‰¹å¾
        s_initial = self.painn.embedding(z_atom) + atom_latent
        
        # é€šè¿‡ PaiNN æå–ç‰¹å¾
        s, v = self.painn(z_atom, vector_features, edge_index, edge_attr, pos, initial_s=s_initial)
        
        # é¢„æµ‹åæ ‡åç§»
        #delta_pos = self.coord_decoder(s)
        # å®ƒçš„ä½œç”¨æ˜¯æŠŠ [N, 3, hidden_dim] çš„å‘é‡ç‰¹å¾å‹ç¼©æˆ [N, 3, 1] çš„ç‰©ç†ä½ç§»
        #self.v_proj = nn.Linear(hidden_dim, 1, bias=False)

        # âœ… å®Œç¾ä¿®å¤ï¼šå°† [N, 128, 3] è½¬ç½®ä¸º [N, 3, 128]
        # è¿™æ ·çº¿æ€§å±‚å°±ä¼šå¯¹ 128 è¿›è¡Œè®¡ç®—ï¼Œè¾“å‡º [N, 3, 1]
        # æœ€å squeeze(-1) æŒ¤æ‰æœ€åé‚£ä¸ª 1ï¼Œç•™ä¸‹å®Œç¾çš„ [N, 3] åæ ‡åç§»ï¼
        delta_pos = self.v_proj(v.transpose(1, 2)).squeeze(-1)
        
        return delta_pos


class GlueVAE(nn.Module):
    """
    GlueVAE ä¸»æ¨¡å‹ã€‚
    å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        latent_dim=32,
        num_encoder_layers=6,
        num_decoder_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨
        self.encoder = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_encoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        
        
        # ================= ğŸš¨ æ–°å¢ï¼šå¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ =================
        self.projector = Projector(hidden_dim=hidden_dim, proj_dim=128)
        # =========================================================
        
        
        
        # è§£ç å™¨
        self.decoder = ConditionalPaiNNDecoder(
            hidden_dim=hidden_dim,
            num_layers=num_decoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )

        # ================= ğŸš¨ æ–°å¢ RBF å±‚ =================
        # edge_dim (19) - æ‹“æ‰‘ç‰¹å¾ (3) = 16 ç»´çš„é«˜æ–¯ç‰¹å¾
        self.rbf = GaussianSmearing(
            start=0.0, 
            stop=10.0, 
            num_gaussians=edge_dim - 3
        )
        # ==================================================
        
    
            
    def encode(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos
    ):
        """å…¨åŸå­ç¼–ç ï¼Œç›´æ¥å°†åŸå­ç‰¹å¾æŠ•å°„åˆ°å¯¹æ¯”ç©ºé—´ã€‚"""
        # 1. PaiNN æå–å…¨åŸå­ç‰¹å¾
        s, v = self.encoder(z, vector_features, edge_index, edge_attr, pos)

        # 2. ğŸš¨ ç›´æ¥ç”¨å…¨åŸå­ç‰¹å¾è¿›è¡ŒæŠ•å½±ï¼Œå½»åº•æŠ›å¼ƒæ®‹åŸºé™ç»´ï¼
        z_proj = self.projector(s) # [N_atoms, proj_dim]

        return s, z_proj

    def decode(
        self,
        atom_features,        # ğŸ‘ˆ ç›´æ¥æ¥æ”¶å…¨åŸå­ç‰¹å¾
        z_atom,
        fake_vector_features, 
        edge_index,
        fake_edge_attr,       
        fake_pos
    ):
        """å…¨åŸå­è§£ç ã€‚"""
        # ç›´æ¥é€šè¿‡è§£ç å™¨é¢„æµ‹åæ ‡åç§»
        delta_pos = self.decoder(
            atom_features, z_atom, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos
        )
        pos_pred = fake_pos + delta_pos
        return pos_pred

    def forward(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        is_ligand,            # ğŸ‘ˆ ğŸš¨ å¿…é¡»æ–°å¢ï¼ç”¨äºåŒºåˆ†å—ä½“(0)å’Œé…ä½“(1)
        mask_interface=None,  
        batch_idx=None        
    ):
        if batch_idx is None or mask_interface is None:
            raise ValueError("CMAE requires batch_idx and mask_interface.")


        num_graphs = int(batch_idx.max().item()) + 1

        # ================= 1. æ„é€  View 1 (Mask A) å’Œ View 2 (Mask B) =================
        # å…‹éš†åæ ‡ï¼Œé˜²æ­¢æ±¡æŸ“åŸå§‹çœŸå®åæ ‡
        pos_v1 = pos.clone() 
        pos_v2 = pos.clone() 

        mask_v1 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)
        mask_v2 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)

        # ğŸš¨ ç»ˆæä¿®å¤ï¼šç§»é™¤äº† if self.training:ï¼Œä¿è¯éªŒè¯æ—¶ä¹Ÿå¿…é¡»ç»å†ç›¸åŒçš„ä¸¥è‹›ç ´åï¼
        for i in range(num_graphs):
            graph_mask = (batch_idx == i)

            # æå– A ä¾§ (å—ä½“, 0) å’Œ B ä¾§ (é…ä½“, 1) çš„ç•Œé¢åŸå­
            interface_A = torch.where(graph_mask & (is_ligand == 0) & (mask_interface == 1))[0]
            interface_B = torch.where(graph_mask & (is_ligand == 1) & (mask_interface == 1))[0]

            # --- ğŸ’¥ View 1: åœ¨ A ä¾§ (å—ä½“) ç‚¸å‡ºä¸€ä¸ª 10 åŸƒçš„å¤§æ´ï¼Œä¿ç•™ B ä¾§ ---
            if len(interface_A) > 0:
                # ğŸš¨ åŠ å…¥ç¡®å®šæ€§åˆ†æ”¯ï¼šè®­ç»ƒæ—¶éšæœºç‚¸ï¼ŒéªŒè¯æ—¶å›ºå®šç‚¸ç¬¬ä¸€ä¸ªä¸­å¿ƒï¼Œä¿è¯è¯„ä¼°å…¬å¹³ï¼
                if self.training:
                    idx_A = torch.randint(0, len(interface_A), (1,))
                else:
                    idx_A = torch.tensor([0], device=pos.device)
                
                center_idx_A = interface_A[idx_A]
                dist_to_center_A = torch.norm(pos[graph_mask] - pos[center_idx_A], p=2, dim=-1)
                local_mask_A = (dist_to_center_A < 10.0) & (is_ligand[graph_mask] == 0)
                global_mask_A = torch.where(graph_mask)[0][local_mask_A]
                mask_v1[global_mask_A] = True

            # --- ğŸ’¥ View 2: åœ¨ B ä¾§ (é…ä½“) ç‚¸å‡ºä¸€ä¸ª 10 åŸƒçš„å¤§æ´ï¼Œä¿ç•™ A ä¾§ ---
            if len(interface_B) > 0:
                # ğŸš¨ åŠ å…¥ç¡®å®šæ€§åˆ†æ”¯ï¼šè®­ç»ƒæ—¶éšæœºç‚¸ï¼ŒéªŒè¯æ—¶å›ºå®šç‚¸ç¬¬ä¸€ä¸ªä¸­å¿ƒ
                if self.training:
                    idx_B = torch.randint(0, len(interface_B), (1,))
                else:
                    idx_B = torch.tensor([0], device=pos.device)

                center_idx_B = interface_B[idx_B]
                dist_to_center_B = torch.norm(pos[graph_mask] - pos[center_idx_B], p=2, dim=-1)
                local_mask_B = (dist_to_center_B < 10.0) & (is_ligand[graph_mask] == 1)
                global_mask_B = torch.where(graph_mask)[0][local_mask_B]
                mask_v2[global_mask_B] = True

        # å®æ–½ç‰©ç†åæ ‡å¡Œé™· (ç»™è¢«ç ´åçš„åŸå­èµ‹äºˆéšæœºé«˜æ–¯å™ªå£°)
        if mask_v1.sum() > 0:
            pos_v1[mask_v1] = torch.randn((mask_v1.sum(), 3), device=pos.device) * 0.5
        if mask_v2.sum() > 0:
            pos_v2[mask_v2] = torch.randn((mask_v2.sum(), 3), device=pos.device) * 0.5
        
        

        # ================= 2. é‡æ–°è®¡ç®—å‡åæ ‡çš„è¾¹ç‰¹å¾ (è·ç¦» RBF) =================
        edge_type = edge_attr[:, :3]
        row, col = edge_index
        fake_vector_features = torch.zeros_like(vector_features) # å…¨é›¶é˜²æ­¢æ³„éœ²

        # View 1 çš„ RBF ç‰¹å¾
        fake_diff_v1 = pos_v1[row] - pos_v1[col]
        fake_dist_v1 = torch.sqrt((fake_diff_v1 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v1 = torch.cat([edge_type, self.rbf(fake_dist_v1)], dim=-1)

        # View 2 çš„ RBF ç‰¹å¾
        fake_diff_v2 = pos_v2[row] - pos_v2[col]
        fake_dist_v2 = torch.sqrt((fake_diff_v2 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v2 = torch.cat([edge_type, self.rbf(fake_dist_v2)], dim=-1)

        # ================= 3. å…¨åŸå­åŒè·¯ç¼–ç  (Encoder) =================
        # æ³¨æ„ï¼šä¸å†ä¼ å…¥ residue_index
        atom_feat_v1, z_proj_v1 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v1, pos_v1)
        atom_feat_v2, z_proj_v2 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v2, pos_v2)

        # ================= ğŸš¨ å‡åç‰ˆï¼šå…¨è¡¥ä¸äº¤å‰æ± åŒ– (æœç» Oracle æ³„éœ²) =================
        # ä»¥å‰ï¼šmask_ligand_interface = (is_ligand == 1) & (mask_interface == 1)
        # ç°åœ¨ï¼šæ¨¡å‹å¿…é¡»è‡ªå·±ä»æ•´ä¸ª Patch ä¸­æå–ç‰¹å¾ï¼Œä¸çŸ¥é“å“ªé‡Œæ˜¯çœŸå®çš„ 4Ã… æ¥è§¦é¢ï¼
        
        # View 1 (Mask A): å—ä½“è¢«ç‚¸æ¯ã€‚Z1 æå–ã€æ•´ä¸ªé…ä½“ Patchã€‘(é’¥åŒ™)
        mask_ligand = (is_ligand == 1)
        z1_patch = z_proj_v1[mask_ligand]
        batch_z1 = batch_idx[mask_ligand]
        graph_z1 = scatter_mean(z1_patch, batch_z1, dim=0, dim_size=num_graphs)

        # View 2 (Mask B): é…ä½“è¢«ç‚¸æ¯ã€‚Z2 æå–ã€æ•´ä¸ªå—ä½“ Patchã€‘(é”å­”)
        mask_receptor = (is_ligand == 0)
        z2_patch = z_proj_v2[mask_receptor]
        batch_z2 = batch_idx[mask_receptor]
        graph_z2 = scatter_mean(z2_patch, batch_z2, dim=0, dim_size=num_graphs)
        # ========================================================================
        # ================= 4. è§£ç é‡æ„ (Decoder) =================
        pos_pred_v1 = self.decode(
            atom_feat_v1, z, fake_vector_features,
            edge_index, fake_edge_attr_v1, pos_v1
        )

        return graph_z1, graph_z2, pos_pred_v1, mask_v1