
"""
Geminiä¸»åˆ€ä¿®æ”¹çš„cmaeæ¶æ„ï¼Œfrom glue_vae_solo.py
GlueVAE ä¸»æ¨¡å‹æ¶æ„ã€‚
å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢ç”Ÿæˆã€‚

æ¶æ„æ¦‚è¿°ï¼š
1. ç¼–ç å™¨ï¼šå¤šå±‚ PaiNNï¼Œæå–å…¨åŸå­ç‰¹å¾
2. ç“¶é¢ˆå±‚ï¼šåŸå­ -&gt; æ®‹åŸº Poolingï¼Œé™ç»´åˆ°æ®‹åŸºçº§åˆ«
3. æ½œåœ¨ç©ºé—´ï¼šé‡å‚æ•°åŒ–é‡‡æ ·
4. è§£ç å™¨ï¼šæ¡ä»¶ç”Ÿæˆï¼Œæ®‹åŸº -&gt; åŸå­ super-resolution
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_mean, scatter_sum

from src.models.layers_solo import PaiNNEncoder
from src.utils.loss_solo import CoordinateDecoder

# ================= ğŸš¨ æ–°å¢ RBF ç±» =================
class GaussianSmearing(nn.Module):
    """
    å¾„å‘åŸºå‡½æ•° (RBF) å±•å¼€ï¼Œç”¨äºå°†æ ‡é‡è·ç¦»æ˜ å°„ä¸ºé«˜ç»´å‘é‡ã€‚
    """
    def __init__(self, start=0.0, stop=10.0, num_gaussians=16):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        # è®¡ç®—é«˜æ–¯å‡½æ•°çš„å®½åº¦ç³»æ•°
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer('offset', offset)

    def forward(self, dist):
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ===================================================

class ResiduePooling(nn.Module):
    """
    åŸå­åˆ°æ®‹åŸºçš„ Pooling å±‚ã€‚
    ä½¿ç”¨ scatter_mean å°†åŒä¸€æ®‹åŸºçš„åŸå­ç‰¹å¾èšåˆä¸ºæ®‹åŸºç‰¹å¾ã€‚
    """
    
    def __init__(self, reduce='mean'):
        super().__init__()
        self.reduce = reduce
        
    def forward(
        self,
        atom_features,
        residue_index
    ):
        """
        å‚æ•°:
            atom_features: [N, hidden_dim] åŸå­çº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾ï¼ŒR ä¸ºæ®‹åŸºæ•°
        """
        if self.reduce == 'mean':
            return scatter_mean(atom_features, residue_index, dim=0)
        elif self.reduce == 'sum':
            return scatter_sum(atom_features, residue_index, dim=0)
        else:
            raise ValueError(f"Unknown reduce type: {self.reduce}")


class ResidueToAtomUnpooling(nn.Module):
    """
    æ®‹åŸºåˆ°åŸå­çš„ Unpooling å±‚ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾å¹¿æ’­å›åŸå­çº§åˆ«ã€‚
    """
    
    def forward(
        self,
        residue_features,
        residue_index
    ):
        """
        å‚æ•°:
            residue_features: [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, hidden_dim] åŸå­çº§ç‰¹å¾
        """
        return residue_features[residue_index]


class Projector(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ (Projection Head)ã€‚
    3å±‚ MLPï¼Œæœ€åè¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œå°†ç‰¹å¾æ˜ å°„åˆ°å¯¹æ¯”ç©ºé—´ã€‚
    """
    def __init__(self, hidden_dim, proj_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, proj_dim)
        )

    def forward(self, x):
        """
        å‚æ•°:
            x: [R, hidden_dim] æ®‹åŸºç‰¹å¾
        è¿”å›:
            z: [R, proj_dim] L2å½’ä¸€åŒ–åçš„å¯¹æ¯”å‘é‡
        """
        z = self.mlp(x)
        # ğŸš¨ æå…¶å…³é”®çš„ä¸€æ­¥ï¼šå¯¹ç‰¹å¾è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œä½¿å…¶åˆ†å¸ƒåœ¨è¶…çƒé¢ä¸Š
        z = F.normalize(z, p=2, dim=-1)
        return z


class ConditionalPaiNNDecoder(nn.Module):
    """
    æ¡ä»¶ PaiNN è§£ç å™¨ã€‚
    ç»“åˆå—ä½“åŸå­å’Œæ½œåœ¨ä¿¡æ¯ï¼Œç”Ÿæˆé…ä½“åŸå­åæ ‡ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        num_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # PaiNN ç¼–ç å™¨ä½œä¸ºè§£ç å™¨ä¸»å¹²
        self.painn = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åæ ‡é¢„æµ‹å¤´
        #self.coord_decoder = CoordinateDecoder(hidden_dim, num_layers=2)
        self.v_proj = nn.Linear(hidden_dim, 1, bias=False)
        
    def forward(
        self,
        atom_latent,
        z_atom,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        å‚æ•°:
            atom_latent: [N, hidden_dim] åŸå­çº§æ½œåœ¨ç‰¹å¾
            z_atom: [N] åŸå­åºæ•°
            vector_features: [N, 3] å‘é‡ç‰¹å¾
            edge_index: [2, E] è¾¹ç´¢å¼•
            edge_attr: [E, edge_dim] è¾¹ç‰¹å¾
            pos: [N, 3] åˆå§‹åæ ‡ï¼ˆå—ä½“å›ºå®šï¼Œé…ä½“å¯è°ƒæ•´ï¼‰
            residue_index: [N] æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»/æ›´æ–°
        """
        # åˆå§‹åŒ–æ ‡é‡ç‰¹å¾ï¼šåµŒå…¥ + æ½œåœ¨ç‰¹å¾
        s_initial = self.painn.embedding(z_atom) + atom_latent
        
        # é€šè¿‡ PaiNN æå–ç‰¹å¾
        s, v = self.painn(z_atom, vector_features, edge_index, edge_attr, pos, initial_s=s_initial)
        
        # é¢„æµ‹åæ ‡åç§»
        #delta_pos = self.coord_decoder(s)
        # å®ƒçš„ä½œç”¨æ˜¯æŠŠ [N, 3, hidden_dim] çš„å‘é‡ç‰¹å¾å‹ç¼©æˆ [N, 3, 1] çš„ç‰©ç†ä½ç§»
        #self.v_proj = nn.Linear(hidden_dim, 1, bias=False)

        # âœ… å®Œç¾ä¿®å¤ï¼šå°† [N, 128, 3] è½¬ç½®ä¸º [N, 3, 128]
        # è¿™æ ·çº¿æ€§å±‚å°±ä¼šå¯¹ 128 è¿›è¡Œè®¡ç®—ï¼Œè¾“å‡º [N, 3, 1]
        # æœ€å squeeze(-1) æŒ¤æ‰æœ€åé‚£ä¸ª 1ï¼Œç•™ä¸‹å®Œç¾çš„ [N, 3] åæ ‡åç§»ï¼
        delta_pos = self.v_proj(v.transpose(1, 2)).squeeze(-1)
        
        return delta_pos


class GlueVAE(nn.Module):
    """
    GlueVAE ä¸»æ¨¡å‹ã€‚
    å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        latent_dim=32,
        num_encoder_layers=6,
        num_decoder_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨
        self.encoder = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_encoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åŸå­ -&gt; æ®‹åŸº Pooling
        self.residue_pooling = ResiduePooling(reduce='mean')
        
        # ================= ğŸš¨ æ–°å¢ï¼šå¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ =================
        self.projector = Projector(hidden_dim=hidden_dim, proj_dim=128)
        # =========================================================
        
        # æ®‹åŸº -&gt; åŸå­ Unpooling
        self.residue_unpooling = ResidueToAtomUnpooling()
        
        # è§£ç å™¨
        self.decoder = ConditionalPaiNNDecoder(
            hidden_dim=hidden_dim,
            num_layers=num_decoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )

        # ================= ğŸš¨ æ–°å¢ RBF å±‚ =================
        # edge_dim (19) - æ‹“æ‰‘ç‰¹å¾ (3) = 16 ç»´çš„é«˜æ–¯ç‰¹å¾
        self.rbf = GaussianSmearing(
            start=0.0, 
            stop=10.0, 
            num_gaussians=edge_dim - 3
        )
        # ==================================================
        
    
            
    def encode(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index
    ):
        """
        ç¼–ç è¿‡ç¨‹ï¼šæå–æ®‹åŸºç‰¹å¾ï¼Œå¹¶æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´ã€‚
        """
        # 1. PaiNN ç¼–ç å™¨æå–åŸå­ç‰¹å¾
        s, v = self.encoder(z, vector_features, edge_index, edge_attr, pos)

        # 2. åŸå­ -> æ®‹åŸº Pooling
        res_features = self.residue_pooling(s, residue_index)

        # 3. æŠ•å½±åˆ°å¯¹æ¯”ç©ºé—´å¾—åˆ° Z 
        z_proj = self.projector(res_features)

        # è¿”å›ï¼šå®Œæ•´çš„æ®‹åŸºç‰¹å¾(ç»™Decoderé‡å»ºç”¨) å’Œ æŠ•å½±åçš„Z(ç»™InfoNCEç®—Lossç”¨)
        return res_features, z_proj

        
    def decode(
        self,
        res_features,         # ğŸ‘ˆ ä¿®æ”¹ï¼šç›´æ¥æ¥æ”¶å®Œæ•´çš„æ®‹åŸºç‰¹å¾ï¼Œä¸å†éœ€è¦ z_latent
        z_atom,
        fake_vector_features, 
        edge_index,
        fake_edge_attr,       
        fake_pos,             
        residue_index
    ):
        """
        è§£ç è¿‡ç¨‹ï¼šæ®‹åŸºç‰¹å¾ -> è¿˜åŸåæ ‡ã€‚
        """
        # Unpoolingï¼šæ®‹åŸºç‰¹å¾ -> åŸå­ç‰¹å¾
        atom_latent = self.residue_unpooling(res_features, residue_index)
        
        # é€šè¿‡è§£ç å™¨ (æ­¤æ—¶ Decoder åªèƒ½çœ‹åˆ°æ®‹ç¼ºçš„ fake_pos å’Œ fake ç‰¹å¾)
        delta_pos = self.decoder(
            atom_latent, z_atom, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos, residue_index
        )
        
        # å¿…é¡»æ˜¯åœ¨ fake_pos çš„åŸºç¡€ä¸Šè¿›è¡Œåç§»ï¼
        pos_pred = fake_pos + delta_pos
        
        return pos_pred

    def forward(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        is_ligand,            # ğŸ‘ˆ ğŸš¨ å¿…é¡»æ–°å¢ï¼ç”¨äºåŒºåˆ†å—ä½“(0)å’Œé…ä½“(1)
        mask_interface=None,  
        batch_idx=None        
    ):
        if batch_idx is None or mask_interface is None:
            raise ValueError("CMAE requires batch_idx and mask_interface.")

        # ================= ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šå…¨å±€å®‰å…¨çš„æ®‹åŸºç´¢å¼•å‹ç¼© =================
        # 1. èµ‹äºˆæ¯ä¸ª Graph æå¤§çš„åç§»é‡ (100000)ï¼Œå½»åº•éš”ç¦»ä¸åŒå¤åˆç‰©çš„æ®‹åŸº ID
        global_residue_index = residue_index + batch_idx * 100000
        # 2. å¯¹è¿™ä¸ªå…¨å±€å®‰å…¨çš„ ID è¿›è¡Œå‹ç¼©æ˜ å°„ï¼Œä¿è¯ç»å¯¹ä¸ä¼šå‘ç”Ÿè·¨ Graph èåˆï¼
        _, residue_index_compact = torch.unique(global_residue_index, sorted=True, return_inverse=True)
        # ======================================================================

        num_graphs = int(batch_idx.max().item()) + 1

        # ================= 1. æ„é€  View 1 (Mask A) å’Œ View 2 (Mask B) =================
        # å…‹éš†åæ ‡ï¼Œé˜²æ­¢æ±¡æŸ“åŸå§‹çœŸå®åæ ‡
        pos_v1 = pos.clone() 
        pos_v2 = pos.clone() 

        mask_v1 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)
        mask_v2 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)

        if self.training:
            for i in range(num_graphs):
                graph_mask = (batch_idx == i)

                # æå– A ä¾§ (å—ä½“, 0) å’Œ B ä¾§ (é…ä½“, 1) çš„ç•Œé¢åŸå­
                interface_A = torch.where(graph_mask & (is_ligand == 0) & (mask_interface == 1))[0]
                interface_B = torch.where(graph_mask & (is_ligand == 1) & (mask_interface == 1))[0]

                # --- ğŸ’¥ View 1: åœ¨ A ä¾§ (å—ä½“) ç‚¸å‡ºä¸€ä¸ª 10 åŸƒçš„å¤§æ´ï¼Œä¿ç•™ B ä¾§ ---
                if len(interface_A) > 0:
                    center_idx_A = interface_A[torch.randint(0, len(interface_A), (1,))]
                    dist_to_center_A = torch.norm(pos[graph_mask] - pos[center_idx_A], p=2, dim=-1)
                    # æ‰¾å‡ºå±€éƒ¨ 10 åŸƒå†…çš„ A ä¾§åŸå­ (å¿…é¡»åŒå±å—ä½“)
                    local_mask_A = (dist_to_center_A < 10.0) & (is_ligand[graph_mask] == 0)
                    global_mask_A = torch.where(graph_mask)[0][local_mask_A]
                    mask_v1[global_mask_A] = True

                # --- ğŸ’¥ View 2: åœ¨ B ä¾§ (é…ä½“) ç‚¸å‡ºä¸€ä¸ª 10 åŸƒçš„å¤§æ´ï¼Œä¿ç•™ A ä¾§ ---
                if len(interface_B) > 0:
                    center_idx_B = interface_B[torch.randint(0, len(interface_B), (1,))]
                    dist_to_center_B = torch.norm(pos[graph_mask] - pos[center_idx_B], p=2, dim=-1)
                    # æ‰¾å‡ºå±€éƒ¨ 10 åŸƒå†…çš„ B ä¾§åŸå­ (å¿…é¡»åŒå±é…ä½“)
                    local_mask_B = (dist_to_center_B < 10.0) & (is_ligand[graph_mask] == 1)
                    global_mask_B = torch.where(graph_mask)[0][local_mask_B]
                    mask_v2[global_mask_B] = True

            # å®æ–½ç‰©ç†åæ ‡å¡Œé™· (ç»™è¢«ç ´åçš„åŸå­èµ‹äºˆéšæœºé«˜æ–¯å™ªå£°ï¼Œå½»åº•å‰¥å¤ºå…¶å±€éƒ¨ç©ºé—´ä¿¡æ¯)
            if mask_v1.sum() > 0:
                pos_v1[mask_v1] = torch.randn((mask_v1.sum(), 3), device=pos.device) * 0.1
            if mask_v2.sum() > 0:
                pos_v2[mask_v2] = torch.randn((mask_v2.sum(), 3), device=pos.device) * 0.1

        # ================= 2. é‡æ–°è®¡ç®—å‡åæ ‡çš„è¾¹ç‰¹å¾ (è·ç¦» RBF) =================
        edge_type = edge_attr[:, :3]
        row, col = edge_index
        fake_vector_features = torch.zeros_like(vector_features) # å…¨é›¶é˜²æ­¢æ³„éœ²

        # View 1 çš„ RBF ç‰¹å¾
        fake_diff_v1 = pos_v1[row] - pos_v1[col]
        fake_dist_v1 = torch.sqrt((fake_diff_v1 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v1 = torch.cat([edge_type, self.rbf(fake_dist_v1)], dim=-1)

        # View 2 çš„ RBF ç‰¹å¾
        fake_diff_v2 = pos_v2[row] - pos_v2[col]
        fake_dist_v2 = torch.sqrt((fake_diff_v2 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v2 = torch.cat([edge_type, self.rbf(fake_dist_v2)], dim=-1)

        # ================= 3. åŒè·¯ç¼–ç  (Encoder) =================
        # ğŸš¨ å¿…é¡»æŠŠå‹ç¼©åçš„ residue_index_compact ä¼ è¿›å»ï¼
        res_feat_v1, z_proj_v1 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v1, pos_v1, residue_index_compact)
        res_feat_v2, z_proj_v2 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v2, pos_v2, residue_index_compact)

        # ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šæ›´å®‰å…¨åœ°æ„é€  res_batch 
        R = int(residue_index_compact.max().item()) + 1
        res_batch = torch.zeros(R, dtype=torch.long, device=pos.device)
        # å› ä¸ºåŒæ®‹åŸºæ‰€æœ‰åŸå­çš„ batch_idx å®Œå…¨ä¸€è‡´ï¼Œç›´æ¥ scatter_ è¦†ç›–èµ‹å€¼
        res_batch.scatter_(0, residue_index_compact, batch_idx)

        if self.training:
            # å·¥ä¸šçº§ä¿é™©ï¼šæ£€æŸ¥åŒä¸€ residue æ˜¯å¦å‡ºç°å¤šä¸ª batch_id
            assert torch.all(res_batch[residue_index_compact] == batch_idx), "Residue spans multiple graphs!"

        # 2. å°†åŒä¸€ä¸ª Graph ä¸‹çš„æ‰€æœ‰æ®‹åŸºå‘é‡åšå¹³å‡
        graph_z1 = scatter_mean(z_proj_v1, res_batch, dim=0)
        graph_z2 = scatter_mean(z_proj_v2, res_batch, dim=0)

        # 3. å†æ¬¡ L2 å½’ä¸€åŒ–
        graph_z1 = F.normalize(graph_z1, p=2, dim=-1)
        graph_z2 = F.normalize(graph_z2, p=2, dim=-1)

        # ================= 4. è§£ç é‡æ„ (Decoder) =================
        # ä¸ºäº†èŠ‚çº¦ç®—åŠ›ä¸”è¾¾åˆ°è¾…åŠ©é‡æ„çš„ç›®çš„ï¼Œæˆ‘ä»¬åªæŒ‘ View 1 è¿›è¡Œè§£ç é‡æ„ã€‚
        # Decoder å¿…é¡»é€šè¿‡éšç©ºé—´ï¼ŒæŠŠè¢«ç‚¸æ‰çš„å—ä½“åæ ‡çŒœå‡ºæ¥ã€‚
        pos_pred_v1 = self.decode(
            res_feat_v1, z, fake_vector_features,
            edge_index, fake_edge_attr_v1, pos_v1, residue_index_compact
        )

        return graph_z1, graph_z2, pos_pred_v1, mask_v1