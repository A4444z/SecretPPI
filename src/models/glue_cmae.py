
"""
Geminiä¸»åˆ€ä¿®æ”¹çš„cmaeæ¶æ„ï¼Œfrom glue_vae_solo.py
GlueVAE ä¸»æ¨¡å‹æ¶æ„ã€‚
å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨ç•Œé¢ç”Ÿæˆã€‚

æ¶æ„æ¦‚è¿°ï¼š
1. ç¼–ç å™¨ï¼šå¤šå±‚ PaiNNï¼Œæå–å…¨åŸå­ç‰¹å¾
2. ç“¶é¢ˆå±‚ï¼šåŸå­ -&gt; æ®‹åŸº Poolingï¼Œé™ç»´åˆ°æ®‹åŸºçº§åˆ«
3. æ½œåœ¨ç©ºé—´ï¼šé‡å‚æ•°åŒ–é‡‡æ ·
4. è§£ç å™¨ï¼šæ¡ä»¶ç”Ÿæˆï¼Œæ®‹åŸº -&gt; åŸå­ super-resolution
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_mean, scatter_sum

from src.models.layers_solo import PaiNNEncoder
from src.utils.loss_solo import CoordinateDecoder

from torch_geometric.utils import softmax

# ================= ğŸš¨ æ–°å¢ RBF ç±» =================
class GaussianSmearing(nn.Module):
    """
    å¾„å‘åŸºå‡½æ•° (RBF) å±•å¼€ï¼Œç”¨äºå°†æ ‡é‡è·ç¦»æ˜ å°„ä¸ºé«˜ç»´å‘é‡ã€‚
    """
    def __init__(self, start=0.0, stop=10.0, num_gaussians=16):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        # è®¡ç®—é«˜æ–¯å‡½æ•°çš„å®½åº¦ç³»æ•°
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer('offset', offset)

    def forward(self, dist):
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ===================================================

class ResiduePooling(nn.Module):
    """
    åŸå­åˆ°æ®‹åŸºçš„ Pooling å±‚ã€‚
    ä½¿ç”¨ scatter_mean å°†åŒä¸€æ®‹åŸºçš„åŸå­ç‰¹å¾èšåˆä¸ºæ®‹åŸºç‰¹å¾ã€‚
    """
    
    def __init__(self, reduce='mean'):
        super().__init__()
        self.reduce = reduce
        
    def forward(
        self,
        atom_features,
        residue_index
    ):
        """
        å‚æ•°:
            atom_features: [N, hidden_dim] åŸå­çº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾ï¼ŒR ä¸ºæ®‹åŸºæ•°
        """
        if self.reduce == 'mean':
            return scatter_mean(atom_features, residue_index, dim=0)
        elif self.reduce == 'sum':
            return scatter_sum(atom_features, residue_index, dim=0)
        else:
            raise ValueError(f"Unknown reduce type: {self.reduce}")


class ResidueToAtomUnpooling(nn.Module):
    """
    æ®‹åŸºåˆ°åŸå­çš„ Unpooling å±‚ã€‚
    å°†æ®‹åŸºçº§ç‰¹å¾å¹¿æ’­å›åŸå­çº§åˆ«ã€‚
    """
    
    def forward(
        self,
        residue_features,
        residue_index
    ):
        """
        å‚æ•°:
            residue_features: [R, hidden_dim] æ®‹åŸºçº§ç‰¹å¾
            residue_index: [N] æ¯ä¸ªåŸå­æ‰€å±çš„æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, hidden_dim] åŸå­çº§ç‰¹å¾
        """
        return residue_features[residue_index]


class Projector(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ (Projection Head)ã€‚
    3å±‚ MLPï¼Œæœ€åè¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œå°†ç‰¹å¾æ˜ å°„åˆ°å¯¹æ¯”ç©ºé—´ã€‚
    """
    def __init__(self, hidden_dim, proj_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, proj_dim)
        )

    def forward(self, x):
        """
        å‚æ•°:
            x: [R, hidden_dim] æ®‹åŸºç‰¹å¾
        è¿”å›:
            z: [R, proj_dim] L2å½’ä¸€åŒ–åçš„å¯¹æ¯”å‘é‡
        """
        z = self.mlp(x)
        # ğŸš¨ æå…¶å…³é”®çš„ä¸€æ­¥ï¼šå¯¹ç‰¹å¾è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œä½¿å…¶åˆ†å¸ƒåœ¨è¶…çƒé¢ä¸Š
        z = F.normalize(z, p=2, dim=-1)
        return z

class MultiHeadAttentionPooling(nn.Module):
    """
    å·¥ä¸šçº§å¤šå¤´æ³¨æ„åŠ›æ± åŒ–å±‚ (å¸¦ LayerNorm å’Œ ç†µæ­£åˆ™åŒ–)ã€‚
    ä¿æŒè¾“å‡ºç»´åº¦ä¸è¾“å…¥ç›¸åŒï¼Œé€šè¿‡åˆ†ç»„ç‰¹å¾å®ç°å¤šå¤´ã€‚
    """
    def __init__(self, hidden_dim=128, num_heads=4):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        if hidden_dim % num_heads != 0:
            raise ValueError("hidden_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤")
        
        self.head_dim = hidden_dim // num_heads
        
        # 1. é¢„å¤„ç†ç¨³å®šå±‚ï¼šé˜²æ­¢åŸå­ç‰¹å¾æå€¼å¯¼è‡´ Softmax å´©å¡Œ
        self.norm = nn.LayerNorm(hidden_dim)
        
        # 2. å¤šå¤´æ‰“åˆ†å™¨ï¼šä¸€æ¬¡æ€§è¾“å‡º num_heads ä¸ªåˆ†æ•°
        self.attn_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, num_heads) # [N, num_heads]
        )

    def forward(self, x, batch, num_graphs):
        N = x.size(0)
        
        # 1. å½’ä¸€åŒ–ä¸æ‰“åˆ†
        x_norm = self.norm(x)
        logits = self.attn_mlp(x_norm) # [N, num_heads]
        
        # 2. è®¡ç®—æ¯ä¸ª Graph å†…éƒ¨çš„æƒé‡
        weights = torch.zeros_like(logits)
        for h in range(self.num_heads):
            weights[:, h] = softmax(logits[:, h], batch, num_nodes=num_graphs, dim=0)
            
        # 3. ğŸš¨ ä¿®å¤ï¼šè®¡ç®—æ³¨æ„åŠ›ç†µå¹¶æŒ‰ Graph æ•°é‡å½’ä¸€åŒ–
        eps = 1e-8
        # å…ˆæ±‚æ‰€æœ‰åŸå­çš„ç†µæ€»å’Œï¼Œç„¶åé™¤ä»¥ Graph æ•°é‡ï¼Œå¾—åˆ°â€œå¹³å‡æ¯ä¸ªå¤åˆç‰©çš„ç†µâ€
        entropy = -torch.sum(weights * torch.log(weights + eps), dim=0) / num_graphs # [num_heads]
        mean_entropy = entropy.mean() # æ ‡é‡
        
        # 4. å¤šå¤´åŠ æƒèšåˆ
        x_split = x.view(N, self.num_heads, self.head_dim)
        weights_expanded = weights.unsqueeze(-1)
        x_weighted = x_split * weights_expanded
        
        x_weighted_flat = x_weighted.view(N, self.hidden_dim)
        graph_z = scatter_sum(x_weighted_flat, batch, dim=0, dim_size=num_graphs) # [B, hidden_dim]
        
        return graph_z, weights, mean_entropy

class ConditionalPaiNNDecoder(nn.Module):
    """
    æ¡ä»¶ PaiNN è§£ç å™¨ã€‚
    ç»“åˆå—ä½“åŸå­å’Œæ½œåœ¨ä¿¡æ¯ï¼Œç”Ÿæˆé…ä½“åŸå­åæ ‡ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        num_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # PaiNN ç¼–ç å™¨ä½œä¸ºè§£ç å™¨ä¸»å¹²
        self.painn = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        # åæ ‡é¢„æµ‹å¤´
        #self.coord_decoder = CoordinateDecoder(hidden_dim, num_layers=2)
        self.v_proj = nn.Linear(hidden_dim, 1, bias=False)
        
    def forward(
        self,
        atom_latent,
        z_atom,
        vector_features,
        edge_index,
        edge_attr,
        pos
    ):
        """
        å‚æ•°:
            atom_latent: [N, hidden_dim] åŸå­çº§æ½œåœ¨ç‰¹å¾
            z_atom: [N] åŸå­åºæ•°
            vector_features: [N, 3] å‘é‡ç‰¹å¾
            edge_index: [2, E] è¾¹ç´¢å¼•
            edge_attr: [E, edge_dim] è¾¹ç‰¹å¾
            pos: [N, 3] åˆå§‹åæ ‡ï¼ˆå—ä½“å›ºå®šï¼Œé…ä½“å¯è°ƒæ•´ï¼‰
            residue_index: [N] æ®‹åŸºç´¢å¼•
        è¿”å›:
            [N, 3] é¢„æµ‹çš„åæ ‡åç§»/æ›´æ–°
        """
        # åˆå§‹åŒ–æ ‡é‡ç‰¹å¾ï¼šåµŒå…¥ + æ½œåœ¨ç‰¹å¾
        s_initial = self.painn.embedding(z_atom) + atom_latent
        
        # é€šè¿‡ PaiNN æå–ç‰¹å¾
        s, v = self.painn(z_atom, vector_features, edge_index, edge_attr, pos, initial_s=s_initial)
        
        # é¢„æµ‹åæ ‡åç§»
        #delta_pos = self.coord_decoder(s)
        # å®ƒçš„ä½œç”¨æ˜¯æŠŠ [N, 3, hidden_dim] çš„å‘é‡ç‰¹å¾å‹ç¼©æˆ [N, 3, 1] çš„ç‰©ç†ä½ç§»
        #self.v_proj = nn.Linear(hidden_dim, 1, bias=False)

        # âœ… å®Œç¾ä¿®å¤ï¼šå°† [N, 128, 3] è½¬ç½®ä¸º [N, 3, 128]
        # è¿™æ ·çº¿æ€§å±‚å°±ä¼šå¯¹ 128 è¿›è¡Œè®¡ç®—ï¼Œè¾“å‡º [N, 3, 1]
        # æœ€å squeeze(-1) æŒ¤æ‰æœ€åé‚£ä¸ª 1ï¼Œç•™ä¸‹å®Œç¾çš„ [N, 3] åæ ‡åç§»ï¼
        delta_pos = self.v_proj(v.transpose(1, 2)).squeeze(-1)
        
        return delta_pos


class GlueVAE(nn.Module):
    """
    GlueVAE ä¸»æ¨¡å‹ã€‚
    å®Œæ•´çš„å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ã€‚
    """
    
    def __init__(
        self,
        hidden_dim=128,
        latent_dim=32,
        num_encoder_layers=6,
        num_decoder_layers=4,
        edge_dim=19,
        vocab_size=101,
        use_gradient_checkpointing=False,
        mask_noise=0.5
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.mask_noise = mask_noise
        
        # ç¼–ç å™¨
        self.encoder = PaiNNEncoder(
            hidden_dim=hidden_dim,
            num_layers=num_encoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )
        
        
        
        # ================= ğŸš¨ æ–°å¢ï¼šå¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ =================
        self.projector = Projector(hidden_dim=hidden_dim, proj_dim=128)

        # ğŸ‘‡ æ–°å¢ï¼šå¤šå¤´æ³¨æ„åŠ›æ± åŒ–å±‚
        self.attn_pooling = MultiHeadAttentionPooling(hidden_dim=128, num_heads=4)
        
        # =========================================================
        
        
        
        # è§£ç å™¨
        self.decoder = ConditionalPaiNNDecoder(
            hidden_dim=hidden_dim,
            num_layers=num_decoder_layers,
            edge_dim=edge_dim,
            vocab_size=vocab_size,
            use_gradient_checkpointing=use_gradient_checkpointing
        )

        # ================= ğŸš¨ æ–°å¢ RBF å±‚ =================
        # edge_dim (19) - æ‹“æ‰‘ç‰¹å¾ (3) = 16 ç»´çš„é«˜æ–¯ç‰¹å¾
        self.rbf = GaussianSmearing(
            start=0.0, 
            stop=10.0, 
            num_gaussians=edge_dim - 3
        )
        # ==================================================
        
    
            
    def encode(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos
    ):
        """å…¨åŸå­ç¼–ç ï¼Œç›´æ¥å°†åŸå­ç‰¹å¾æŠ•å°„åˆ°å¯¹æ¯”ç©ºé—´ã€‚"""
        # 1. PaiNN æå–å…¨åŸå­ç‰¹å¾
        s, v = self.encoder(z, vector_features, edge_index, edge_attr, pos)

        # 2. ğŸš¨ ç›´æ¥ç”¨å…¨åŸå­ç‰¹å¾è¿›è¡ŒæŠ•å½±ï¼Œå½»åº•æŠ›å¼ƒæ®‹åŸºé™ç»´ï¼
        z_proj = self.projector(s) # [N_atoms, proj_dim]

        return s, z_proj

    def decode(
        self,
        atom_features,        # ğŸ‘ˆ ç›´æ¥æ¥æ”¶å…¨åŸå­ç‰¹å¾
        z_atom,
        fake_vector_features, 
        edge_index,
        fake_edge_attr,       
        fake_pos
    ):
        """å…¨åŸå­è§£ç ã€‚"""
        # ç›´æ¥é€šè¿‡è§£ç å™¨é¢„æµ‹åæ ‡åç§»
        delta_pos = self.decoder(
            atom_features, z_atom, fake_vector_features,
            edge_index, fake_edge_attr, fake_pos
        )
        pos_pred = fake_pos + delta_pos
        return pos_pred

    def forward(
        self,
        z,
        vector_features,
        edge_index,
        edge_attr,
        pos,
        residue_index,
        is_ligand,            # ğŸ‘ˆ ğŸš¨ å¿…é¡»æ–°å¢ï¼ç”¨äºåŒºåˆ†å—ä½“(0)å’Œé…ä½“(1)
        mask_interface=None,  
        batch_idx=None        
    ):
        if batch_idx is None or mask_interface is None:
            raise ValueError("CMAE requires batch_idx and mask_interface.")


        num_graphs = int(batch_idx.max().item()) + 1

        # ğŸš¨ æ ¸å¿ƒæ‰‹æœ¯ï¼šç‰©ç†æ–©æ–­è·¨é“¾è¾¹ (Sever Cross-chain Edges)
        # å¼ºè¿«æ¨¡å‹åˆ†åˆ«å­¦ä¹ å­¤ç«‹çš„é¶ç‚¹è¡¨é¢å’Œé…ä½“è¡¨é¢ï¼Œæ¶ˆé™¤ OOD (åˆ†å¸ƒåç§») åç¼©
        # ========================================================================
        row, col = edge_index
        # åªæœ‰è¾¹çš„ä¸¤ç«¯å±äºåŒä¸€æ¡é“¾ï¼ˆéƒ½æ˜¯ 0ï¼Œæˆ–éƒ½æ˜¯ 1ï¼‰ï¼Œsame_chain_mask æ‰ä¸º True
        same_chain_mask = (is_ligand[row] == is_ligand[col])
        
        # è¦†ç›–åŸå§‹çš„å›¾æ‹“æ‰‘
        edge_index = edge_index[:, same_chain_mask]
        edge_attr = edge_attr[same_chain_mask, :]

        # ================= 1. æ„é€  View 1 (Mask A) å’Œ View 2 (Mask B) =================
        # å…‹éš†åæ ‡ï¼Œé˜²æ­¢æ±¡æŸ“åŸå§‹çœŸå®åæ ‡
        pos_v1 = pos.clone() 
        pos_v2 = pos.clone() 

        mask_v1 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)
        mask_v2 = torch.zeros(pos.size(0), dtype=torch.bool, device=pos.device)

        # ğŸš¨ ç»ˆæä¿®å¤ï¼šå½»åº•å»é™¤å¯¹ mask_interface çš„ä¾èµ–ï¼Œå®ç°çœŸæ­£çš„è‡ªç›‘ç£æ©è”½ï¼
        for i in range(num_graphs):
            graph_mask = (batch_idx == i)

            # ğŸš¨ ä¿®æ”¹ï¼šæå– A ä¾§å’Œ B ä¾§çš„ã€æ‰€æœ‰ã€‘åŸå­ï¼Œä¸å†é™å®š mask_interface == 1
            atoms_A = torch.where(graph_mask & (is_ligand == 0))[0]
            atoms_B = torch.where(graph_mask & (is_ligand == 1))[0]

            # --- ğŸ’¥ View 1: åœ¨ A ä¾§ (å—ä½“) éšæœºç‚¸å‡ºä¸€ä¸ªå¤§æ´ï¼Œä¿ç•™ B ä¾§ ---
            if len(atoms_A) > 0:
                if self.training:
                    idx_A = torch.randint(0, len(atoms_A), (1,))
                else:
                    pos_A = pos[atoms_A]
                    center_A = pos_A.mean(dim=0, keepdim=True)
                    idx_A = torch.argmin(torch.norm(pos_A - center_A, dim=-1)).view(1) # ç¡®å®šæ€§
                
                center_idx_A = atoms_A[idx_A]
                dist_to_center_A = torch.norm(pos[graph_mask] - pos[center_idx_A], p=2, dim=-1)
                local_mask_A = (dist_to_center_A < 10.0) & (is_ligand[graph_mask] == 0)
                global_mask_A = torch.where(graph_mask)[0][local_mask_A]
                mask_v1[global_mask_A] = True

            # --- ğŸ’¥ View 2: åœ¨ B ä¾§ (é…ä½“) éšæœºç‚¸å‡ºä¸€ä¸ªå¤§æ´ï¼Œä¿ç•™ A ä¾§ ---
            if len(atoms_B) > 0:
                if self.training:
                    idx_B = torch.randint(0, len(atoms_B), (1,))
                else:
                    pos_B = pos[atoms_B]
                    center_B = pos_B.mean(dim=0, keepdim=True)
                    idx_B = torch.argmin(torch.norm(pos_B - center_B, dim=-1)).view(1)

                center_idx_B = atoms_B[idx_B]
                dist_to_center_B = torch.norm(pos[graph_mask] - pos[center_idx_B], p=2, dim=-1)
                local_mask_B = (dist_to_center_B < 10.0) & (is_ligand[graph_mask] == 1)
                global_mask_B = torch.where(graph_mask)[0][local_mask_B]
                mask_v2[global_mask_B] = True

        # å®æ–½ç‰©ç†åæ ‡å¡Œé™· (ç»™è¢«ç ´åçš„åŸå­èµ‹äºˆéšæœºé«˜æ–¯å™ªå£°)
        if mask_v1.sum() > 0:
            pos_v1[mask_v1] = torch.randn((mask_v1.sum(), 3), device=pos.device) * self.mask_noise
        if mask_v2.sum() > 0:
            pos_v2[mask_v2] = torch.randn((mask_v2.sum(), 3), device=pos.device) * self.mask_noise
        
        

        # ================= 2. é‡æ–°è®¡ç®—å‡åæ ‡çš„è¾¹ç‰¹å¾ (è·ç¦» RBF) =================
        edge_type = edge_attr[:, :3]
        row, col = edge_index
        fake_vector_features = torch.zeros_like(vector_features) # å…¨é›¶é˜²æ­¢æ³„éœ²

        # View 1 çš„ RBF ç‰¹å¾
        fake_diff_v1 = pos_v1[row] - pos_v1[col]
        fake_dist_v1 = torch.sqrt((fake_diff_v1 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v1 = torch.cat([edge_type, self.rbf(fake_dist_v1)], dim=-1)

        # View 2 çš„ RBF ç‰¹å¾
        fake_diff_v2 = pos_v2[row] - pos_v2[col]
        fake_dist_v2 = torch.sqrt((fake_diff_v2 ** 2).sum(dim=-1) + 1e-8)
        fake_edge_attr_v2 = torch.cat([edge_type, self.rbf(fake_dist_v2)], dim=-1)

        # ================= 3. å…¨åŸå­åŒè·¯ç¼–ç  (Encoder) =================
        # æ³¨æ„ï¼šä¸å†ä¼ å…¥ residue_index
        atom_feat_v1, z_proj_v1 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v1, pos_v1)
        atom_feat_v2, z_proj_v2 = self.encode(z, fake_vector_features, edge_index, fake_edge_attr_v2, pos_v2)

        # ================= ğŸš¨ å‡åç‰ˆï¼šå…¨è¡¥ä¸äº¤å‰æ± åŒ– (æœç» Oracle æ³„éœ²) =================
        # ä»¥å‰ï¼šmask_ligand_interface = (is_ligand == 1) & (mask_interface == 1)
        # ç°åœ¨ï¼šæ¨¡å‹å¿…é¡»è‡ªå·±ä»æ•´ä¸ª Patch ä¸­æå–ç‰¹å¾ï¼Œä¸çŸ¥é“å“ªé‡Œæ˜¯çœŸå®çš„ 4Ã… æ¥è§¦é¢ï¼
        
        # ================= ğŸš¨ ä¿®å¤ 2ï¼šæ³¨æ„åŠ›äº¤å‰æ± åŒ– (Attention Pooling) =================
        mask_ligand = (is_ligand == 1)
        z1_patch = z_proj_v1[mask_ligand]
        batch_z1 = batch_idx[mask_ligand]
        # ç”¨æ³¨æ„åŠ›ä»£æ›¿ scatter_mean
        graph_z1, attn_w1, entropy_1 = self.attn_pooling(z1_patch, batch_z1, num_graphs)

        mask_receptor = (is_ligand == 0)
        z2_patch = z_proj_v2[mask_receptor]
        batch_z2 = batch_idx[mask_receptor]
        # ç”¨æ³¨æ„åŠ›ä»£æ›¿ scatter_mean
        graph_z2, attn_w2, entropy_2 = self.attn_pooling(z2_patch, batch_z2, num_graphs)

        # å†æ¬¡ L2 å½’ä¸€åŒ–
        graph_z1 = F.normalize(graph_z1, p=2, dim=-1, eps=1e-8)
        graph_z2 = F.normalize(graph_z2, p=2, dim=-1, eps=1e-8)
        
        # æ±‡æ€»ç†µ
        batch_entropy = (entropy_1 + entropy_2) / 2.0
        # ========================================================================

        # ================= ğŸš¨ æ–°å¢ï¼šæ³¨æ„åŠ›å¼•å¯¼æŸå¤± (Attention Guidance) =================
        # æå–å½“å‰ Batch çš„çœŸå®ç•Œé¢æ ‡ç­¾
        label_ligand = mask_interface[mask_ligand].float()
        label_receptor = mask_interface[mask_receptor].float()
        
        # å°†å¤šå¤´æ³¨æ„åŠ›æƒé‡ [N, num_heads] å¹³å‡æˆå•å¤´ç»¼åˆæ³¨æ„åŠ›æ¦‚ç‡ [N]
        prob_ligand = attn_w1.mean(dim=-1)
        prob_receptor = attn_w2.mean(dim=-1)
        
        # è®¡ç®—è¾…åŠ©å¼•å¯¼æŸå¤±ï¼šé¼“åŠ± attention æƒé‡åœ¨ mask_interface == 1 çš„åœ°æ–¹å˜å¤§
        # ç›¸å½“äºè®¡ç®—äº¤å‰ç†µçš„æ­£æ ·æœ¬é¡¹ï¼Œé™¤ä»¥çœŸå®ç•Œé¢åŸå­æ•°ä»¥ç¨³å®šé‡çº§
        eps = 1e-8
        guidance_loss_ligand = -torch.sum(label_ligand * torch.log(prob_ligand + eps)) / (label_ligand.sum() + eps)
        guidance_loss_receptor = -torch.sum(label_receptor * torch.log(prob_receptor + eps)) / (label_receptor.sum() + eps)
        
        attn_guidance_loss = (guidance_loss_ligand + guidance_loss_receptor) / 2.0
        # ========================================================================
        
        pos_pred_v1 = self.decode(
            atom_feat_v1, z, fake_vector_features,
            edge_index, fake_edge_attr_v1, pos_v1
        )

        # ğŸ‘‡ ç»“å°¾å¿…é¡»å¤šè¿”å›ä¸€ä¸ª batch_entropy
        return graph_z1, graph_z2, pos_pred_v1, mask_v1, batch_entropy, attn_guidance_loss