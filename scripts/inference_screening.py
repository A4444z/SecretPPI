import os
import sys

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œå¹¶åŠ å…¥ç³»ç»Ÿè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import torch
import torch.nn.functional as F
import numpy as np
from torch_geometric.data import Data, Batch
from torch_geometric.nn.pool import fps
from torch_geometric.nn import radius_graph

from torch_cluster import radius_graph
from torch_scatter import scatter_add
from src.utils.geometry import GaussianRBF

# å¯¼å…¥ä½ çš„æ¨¡å‹
from src.models.glue_cmae import GlueVAE

import warnings
from Bio.PDB import PDBParser
from Bio.PDB.PDBExceptions import PDBConstructionWarning

# å¿½ç•¥ Biopython è§£æ PDB æ—¶çš„çƒ¦äººè­¦å‘Š
warnings.simplefilter('ignore', PDBConstructionWarning)

# å¸¸è§å…ƒç´ ç¬¦å·åˆ°åŸå­åºæ•°çš„æ˜ å°„è¡¨
ELEMENT_TO_Z = {'H': 1, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'P': 15, 'S': 16, 'CL': 17}

def parse_pdb_to_pyg(pdb_path):
    """
    å°† PDB æ–‡ä»¶è§£æä¸º CMAE æ¨¡å‹æ‰€éœ€çš„ PyG Data å¯¹è±¡ã€‚
    """
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("protein", pdb_path)

    pos_list = []
    z_list = []
    residue_indices = []

    res_count = 0
    for model in structure:
        for chain in model:
            for residue in chain:
                # è·³è¿‡æ°´åˆ†å­å’Œå¼‚è´¨åˆ†å­ (HETATM)
                if residue.id[0] != ' ': 
                    continue 
                for atom in residue:
                    pos_list.append(atom.coord)
                    element = atom.element.strip().upper()
                    # è·å–åŸå­åºæ•°ï¼Œå¦‚æœä¸è®¤è¯†é»˜è®¤ç»™ 6 (ç¢³åŸå­)
                    z_list.append(ELEMENT_TO_Z.get(element, 6))
                    residue_indices.append(res_count)
                res_count += 1

    if len(pos_list) == 0:
        raise ValueError(f"ä» {pdb_path} ä¸­æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆåŸå­ï¼")

    # æ„å»º Tensor
    pos = torch.tensor(pos_list, dtype=torch.float32)
    x = torch.tensor(z_list, dtype=torch.long)
    residue_index = torch.tensor(residue_indices, dtype=torch.long)
    
    # é»˜è®¤åŒä¸€æ¡é“¾
    is_ligand = torch.zeros(len(pos), dtype=torch.long)

    # è¿”å›åŸºç¡€æ•°æ®ï¼Œå‘é‡ç‰¹å¾å’Œè¾¹ç‰¹å¾ä¼šåœ¨ screening ä¸­è‡ªåŠ¨æ„å»º
    return Data(x=x, pos=pos, residue_index=residue_index, is_ligand=is_ligand)


def save_patch_with_attention_to_pdb(patch_data, attention_weights, out_path="best_match_patch.pdb"):
    """
    é«˜é˜¶åŠŸèƒ½ï¼šå°†é€‰ä¸­çš„è›‹ç™½è´¨æ–‘å—ä¿å­˜ä¸º PDB æ–‡ä»¶ï¼Œ
    å¹¶å°† Attention æƒé‡å†™å…¥ B-factor åˆ—ï¼Œä»¥ä¾¿åœ¨ PyMOL ä¸­è¿›è¡Œçƒ­åŠ›å›¾å¯è§†åŒ–ï¼
    """
    pos = patch_data.pos.cpu().numpy()
    z_array = patch_data.x.cpu().numpy()
    
    # é€†å‘æ˜ å°„åŸå­åºæ•°åˆ°å…ƒç´ å
    Z_TO_ELEMENT = {v: k for k, v in ELEMENT_TO_Z.items()}
    
    # å½’ä¸€åŒ– attention æƒé‡åˆ° 0~100 (B-factor å¸¸ç”¨çš„èŒƒå›´)
    attn = attention_weights
    if attn.max() > attn.min():
        attn_norm = (attn - attn.min()) / (attn.max() - attn.min()) * 100.0
    else:
        attn_norm = np.zeros_like(attn)

    with open(out_path, 'w') as f:
        for i in range(len(pos)):
            x, y, z = pos[i]
            element = Z_TO_ELEMENT.get(z_array[i], 'C')
            b_factor = attn_norm[i]
            # ä¸¥æ ¼æŒ‰ç…§ PDB æ ¼å¼è§„èŒƒå†™å…¥
            f.write(f"ATOM  {i+1:>5}  CA  ALA A   1    {x:>8.3f}{y:>8.3f}{z:>8.3f}  1.00{b_factor:>6.2f}          {element:>2}\n")
    print(f"ğŸ’¾ å¸¦æœ‰ Attention çƒ­åŠ›å›¾çš„æ–‘å—å·²ä¿å­˜è‡³: {out_path}")

class VirtualScreener:
    def __init__(self, model_path, config, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.config = config
        
        # åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
        print("ğŸ§  æ­£åœ¨åŠ è½½ CMAE ç­›é€‰å™¨...")
        self.model = GlueVAE(
            hidden_dim=config['model']['hidden_dim'],
            num_encoder_layers=config['model']['num_encoder_layers'],
            num_decoder_layers=config['model']['num_decoder_layers'],
            edge_dim=config['model']['edge_dim'],
            vocab_size=config['model']['vocab_size']
        ).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        clean_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}
        self.model.load_state_dict(clean_state_dict)
        self.model.eval()

        # åœ¨ __init__ çš„æœ«å°¾åŠ ä¸Šï¼š
        self.cutoff_radius = 8.0  # å¯¹åº” dataset ä¸­çš„é»˜è®¤æˆªæ–­åŠå¾„
        self.rbf = GaussianRBF(n_rbf=16, cutoff=self.cutoff_radius, start=0.0).to(self.device)

    @torch.no_grad()
    def get_latent_representation(self, patch_data):
        """
        æ ¸å¿ƒæ¨ç†ï¼šç»•è¿‡ Forward æ©ç ï¼Œç›´æ¥è°ƒç”¨ Encoder å’Œ AttnPooling æå– 128 ç»´æµå½¢ç‰¹å¾
        """
        patch_data = patch_data.to(self.device)
        num_graphs = patch_data.batch.max().item() + 1 if hasattr(patch_data, 'batch') else 1
        batch_idx = patch_data.batch if hasattr(patch_data, 'batch') else torch.zeros(patch_data.x.size(0), dtype=torch.long, device=self.device)

        # 1. æå–å…¨åŸå­ç­‰å˜ç‰¹å¾ä¸å¯¹æ¯”æŠ•å½±
        s, z_proj = self.model.encode(
            z=patch_data.x, 
            vector_features=patch_data.vector_features, 
            edge_index=patch_data.edge_index, 
            edge_attr=patch_data.edge_attr, 
            pos=patch_data.pos
        )

        # 2. å¤šå¤´æ³¨æ„åŠ›æ± åŒ–èšåˆä¸º Graph çº§åˆ«å‘é‡
        graph_z, attn_w, _ = self.model.attn_pooling(z_proj, batch_idx, num_graphs)
        
        # 3. L2 å½’ä¸€åŒ–ï¼Œè¿›å…¥ç»å¯¹å¯¹æ¯”ç©ºé—´
        graph_z = F.normalize(graph_z, p=2, dim=-1)
        
        return graph_z, attn_w

    def extract_patch_manual(self, full_protein_data, center_residue_indices, radius=15.0):
        """
        æ–¹æ³•ä¸€ï¼šã€æŒ‡å®šç•Œé¢ã€‘åŸºäºäººå·¥æŒ‡å®šçš„æ®‹åŸºæå– Patch
        center_residue_indices: list or tensor, æŒ‡å®šçš„æ®‹åŸºç´¢å¼•
        """
        # è®¡ç®—æŒ‡å®šæ®‹åŸºçš„å‡ ä½•ä¸­å¿ƒ
        selected_atoms = torch.isin(full_protein_data.residue_index, torch.tensor(center_residue_indices))
        center_coords = full_protein_data.pos[selected_atoms].mean(dim=0, keepdim=True)
        
        # æˆªå–åŠå¾„å†…çš„æ‰€æœ‰åŸå­
        dist = torch.norm(full_protein_data.pos - center_coords, dim=-1)
        patch_mask = dist <= radius
        
        if patch_mask.sum() == 0:
            raise ValueError("æŒ‡å®šçš„æ®‹åŸºé™„è¿‘æ²¡æœ‰æå–åˆ°åŸå­ï¼Œè¯·æ£€æŸ¥åæ ‡æˆ–åŠå¾„ã€‚")
            
        return self._subgraph_from_mask(full_protein_data, patch_mask)

    def extract_patches_auto(self, full_protein_data, num_patches=20, radius=15.0):
        """
        æ–¹æ³•äºŒï¼šã€è‡ªåŠ¨é€‰å–ã€‘åˆ©ç”¨æœ€è¿œç‚¹é‡‡æ · (FPS) åœ¨è›‹ç™½è¡¨é¢å‡åŒ€æ’’ç½‘ï¼Œæå–å¤šä¸ªå€™é€‰ Patch
        è¿”å›: List[Data], åŒ…å«æ‰€æœ‰é‡‡æ ·åˆ°çš„å±€éƒ¨æ–‘å—
        """
        pos = full_protein_data.pos
        
        # ä½¿ç”¨ FPS è·å–åˆ†å¸ƒåœ¨è¡¨é¢/å…¨å±€çš„æœ€è¿œç‚¹é”šç‚¹ç´¢å¼•
        batch_dummy = torch.zeros(pos.size(0), dtype=torch.long, device=pos.device)
        # fps çš„ ratio = num_patches / total_atoms
        ratio = min(1.0, num_patches / pos.size(0))
        anchor_indices = fps(pos, batch_dummy, ratio=ratio)
        
        # å¦‚æœè·å–çš„é”šç‚¹å¤šäºè®¾å®šå€¼ï¼Œæˆªæ–­
        anchor_indices = anchor_indices[:num_patches]
        
        patches = []
        for idx in anchor_indices:
            center_coords = pos[idx].unsqueeze(0)
            dist = torch.norm(pos - center_coords, dim=-1)
            patch_mask = dist <= radius
            
            # è¿‡æ»¤æ‰å¤ªå°çš„ç¢ç‰‡ (æ¯”å¦‚æ¸¸ç¦»çš„å•ä¸ªæ°¨åŸºé…¸)
            if patch_mask.sum() > 10:
                patches.append(self._subgraph_from_mask(full_protein_data, patch_mask))
                
        return patches

    def _subgraph_from_mask(self, data, mask):
        """ä»å®Œæ•´çš„è›‹ç™½å›¾æ•°æ®ä¸­åˆ‡å‰²å‡ºå­å›¾ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ—¶çš„æ–¹å¼é‡æ„è¾¹ä¸å‘é‡ç‰¹å¾"""
        device = data.pos.device
        
        # 1. æå–å­å›¾èŠ‚ç‚¹ç‰¹å¾
        subset_data = Data(
            x=data.x[mask],
            pos=data.pos[mask],
            residue_index=data.residue_index[mask] if hasattr(data, 'residue_index') else None,
            is_ligand=data.is_ligand[mask] if hasattr(data, 'is_ligand') else None
        )
        
        # 2. é‡æ„æˆªæ–­åŠå¾„å›¾ (Radius Graph)
        # ä¸¥æ ¼å¯¹é½ dataset.py ä¸­çš„ self.cutoff_radius (é»˜è®¤ä¸º 8.0)
        edge_index = radius_graph(subset_data.pos, r=self.cutoff_radius, loop=False)
        subset_data.edge_index = edge_index
        
        # 3. è®¡ç®—è¾¹çš„è·ç¦»ä¸æ‹“æ‰‘ç±»å‹
        row, col = edge_index
        diff = subset_data.pos[row] - subset_data.pos[col]
        dist = torch.norm(diff, p=2, dim=-1)
        
        is_covalent = dist < 1.7
        
        # å®¹é”™å¤„ç†ï¼šå¦‚æœåœ¨å•è›‹ç™½æ‰«ææ—¶æ²¡æœ‰ is_ligandï¼Œåˆ™é»˜è®¤éƒ½åœ¨åŒä¸€æ¡é“¾
        if hasattr(subset_data, 'is_ligand') and subset_data.is_ligand is not None:
            same_chain = (subset_data.is_ligand[row] == subset_data.is_ligand[col])
        else:
            same_chain = torch.ones_like(is_covalent, dtype=torch.bool)
            
        edge_type = torch.zeros((edge_index.size(1), 3), dtype=torch.float, device=device)
        edge_type[is_covalent, 0] = 1.0
        edge_type[(~is_covalent) & same_chain, 1] = 1.0
        edge_type[(~is_covalent) & (~same_chain), 2] = 1.0
        
        # 4. æ‹¼æ¥è¾¹ç‰¹å¾ (æ‹“æ‰‘ç‰¹å¾ + RBF è·ç¦»ç‰¹å¾)
        rbf_feat = self.rbf(dist.to(self.device)).to(device)
        subset_data.edge_attr = torch.cat([edge_type, rbf_feat], dim=-1)
        
        # 5. é‡æ–°è®¡ç®—é²æ£’çš„å‘é‡ç‰¹å¾ (Vector Features)
        mask_cov = is_covalent
        row_cov = row[mask_cov]
        col_cov = col[mask_cov]
        
        N = subset_data.pos.size(0)
        vector_features = torch.zeros(N, 3, device=device)
        
        if len(row_cov) > 0:
            vec_diff = subset_data.pos[row_cov] - subset_data.pos[col_cov]
            vector_features = scatter_add(vec_diff, col_cov, dim=0, dim_size=N)
            
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ•…æ„æ²¡æœ‰åŠ éšæœºå™ªå£° (randn)ï¼Œä¸ºäº†ä¿è¯æ¨æ–­æ—¶ SE(3) çš„ç»å¯¹ç¨³å®šæ€§
        subset_data.vector_features = vector_features
        
        # è®°å½• batch ç´¢å¼• (ç”¨äºåç»­çš„ Attention Pooling)
        subset_data.batch = torch.zeros(N, dtype=torch.long, device=device)
        
        return subset_data

    def screen(self, target_patch, candidate_protein_data):
        """
        ç»ˆæç­›é€‰å·¥ä½œæµï¼šæ‹¿ Target é’¥åŒ™å» Candidate è¡¨é¢å¼€é”
        """
        # 1. æå–ç›®æ ‡è›‹ç™½ A (Target) çš„æ ‡å‡†ç‰¹å¾
        print("ğŸ¯ æ­£åœ¨æå–è›‹ç™½ A (Target) çš„æµå½¢ç‰¹å¾...")
        z_target, _ = self.get_latent_representation(target_patch)  # [1, 128]
        
        # 2. å¯¹å€™é€‰è›‹ç™½ B (Candidate) è¿›è¡Œå…¨æ™¯æ‰«æ
        print("ğŸŒ æ­£åœ¨å¯¹è›‹ç™½ B (Candidate) è¿›è¡Œè¡¨é¢è‡ªåŠ¨é‡‡æ ·ä¸ç‰¹å¾ç¼–ç ...")
        candidate_patches = self.extract_patches_auto(candidate_protein_data, num_patches=20)
        print(f"   æå–åˆ° {len(candidate_patches)} ä¸ªæœ‰æ•ˆå€™é€‰æ–‘å—ã€‚")
        
        # å°†å¤šä¸ªå€™é€‰æ–‘å—æ‹¼æˆä¸€ä¸ª Batch æå‡è®¡ç®—æ•ˆç‡
        batch_candidates = Batch.from_data_list(candidate_patches)
        z_candidates, attn_weights = self.get_latent_representation(batch_candidates) # [N_patches, 128]
        
        # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        # z_target å’Œ z_candidates éƒ½å·²ç» L2 å½’ä¸€åŒ–ï¼Œç›´æ¥çŸ©é˜µç›¸ä¹˜å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = torch.matmul(z_candidates, z_target.T).squeeze(-1) # [N_patches]
        
        # 4. å¯»æ‰¾ Best Match
        best_idx = torch.argmax(similarities).item()
        best_score = similarities[best_idx].item()
        best_patch = candidate_patches[best_idx]
        
        # æå–æœ€ä½³æ–‘å—å†…éƒ¨çš„åŸå­çº§ Attention æƒé‡ (æ‰¾åˆ°å…·ä½“æ˜¯è°åœ¨èµ·ä½œç”¨)
        # å› ä¸º attn_weights æ˜¯ [Total_atoms, num_heads]ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‹†å›å¯¹åº”å›¾
        # ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œå‡å®šå–å¹³å‡å¤´éƒ¨çš„æƒé‡
        best_attn_w = attn_weights[batch_candidates.batch == best_idx].mean(dim=-1).cpu().numpy()
        
        print("\n" + "="*40)
        print(f"âœ… ç­›é€‰å®Œæˆï¼")
        print(f"ğŸ† æœ€é«˜åŒ¹é…ç›¸ä¼¼åº¦ (Score): {best_score:.4f}")
        print(f"ğŸ“ æœ€ä½³åŒ¹é…å‘ç”Ÿåœ¨è¯¥æ–‘å—çš„è´¨å¿ƒåæ ‡é™„è¿‘: {best_patch.pos.mean(dim=0).cpu().numpy()}")
        print("="*40)
        
        return {
            'best_score': best_score,
            'best_patch_data': best_patch,
            'best_attention_weights': best_attn_w,
            'all_scores': similarities.cpu().numpy()
        }

if __name__ == "__main__":
    main()