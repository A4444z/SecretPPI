"""
GlueVAE/CMAE åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ (DDP)ã€‚
ä½¿ç”¨ torchrun + DistributedDataParallelï¼Œæ”¯æŒå¤šå¡å¹¶è¡Œè®­ç»ƒã€‚
"""

import os
import sys
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import random_split, DistributedSampler, Subset
from torch_geometric.loader import DataLoader as PyGDataLoader
import yaml
from tqdm import tqdm
from datetime import timedelta
import datetime

# ================= ğŸš¨ æ–°çš„ CMAE å¯¼å…¥ =================
from src.models.glue_cmae import GlueVAE  # å‡è®¾ä½ åœ¨ glue_cmae.py é‡Œç±»åè¿˜æ˜¯å« GlueVAE
from src.utils.loss_cmae import CMAELoss
# ç§»é™¤äº† VAELoss å’Œ BetaScheduler
from src.data.dataset import GlueVAEDataset
# ====================================================

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def load_config(config_path):
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶ã€‚"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def setup_wandb(config, rank):
    """è®¾ç½® wandb æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨ä¸” rank ä»… rank 0ï¼‰ã€‚"""
    if rank == 0 and config['logging']['use_wandb']:
        try:
            import wandb
            wandb.init(
                project=config['logging']['project'],
                entity=config['logging']['entity'],
                config=config
            )
            return wandb
        except ImportError:
            print("Warning: wandb not installed, disabling logging")
            config['logging']['use_wandb'] = False
    return None


def train_epoch(
    model,
    train_loader,
    optimizer,
    criterion,
    device,            # ğŸ‘ˆ åˆ é™¤äº† beta_scheduler
    epoch,
    config,
    rank,
    wandb_logger=None
    ):
    """è®­ç»ƒä¸€ä¸ª epochã€‚"""
    model.train()
    total_loss = 0.0
    total_recon_loss = 0.0
    total_contrast_loss = 0.0  # ğŸ‘ˆ æ›¿æ¢äº† kl_loss
    num_batches = 0
    
    log_interval = config['logging']['log_interval']
    
    if rank == 0:
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    else:
        pbar = train_loader
    
    for batch_idx, batch in enumerate(pbar):
        batch = batch.to(device)
        
        if rank == 0 and epoch == 0 and batch_idx == 0:
            print("\n" + "="*40)
            print("ğŸš€ é¦–æ‰¹æ•°æ®å¥åº·ä½“æ£€æŠ¥å‘Š (CMAEç‰ˆ)")
            print(f"  æ€»åŸå­æ•°: {batch.pos.size(0)}")
            print(f"  [pos] æ˜¯å¦å« NaN/Inf: {torch.isnan(batch.pos).any().item() or torch.isinf(batch.pos).any().item()}")
            print(f"  [pos] æ•°å€¼èŒƒå›´: æœ€å°å€¼ {batch.pos.min().item():.2f}, æœ€å¤§å€¼ {batch.pos.max().item():.2f}")
            print(f"  [is_ligand] æ˜¯å¦å­˜åœ¨: {hasattr(batch, 'is_ligand')}")
            print("="*40 + "\n")
        
        # ================= ğŸš¨ æ–°çš„å‰å‘ä¼ æ’­æ¥å£ =================
        z1, z2, pos_pred_v1, mask_v1, batch_entropy = model(
            z=batch.x,
            vector_features=batch.vector_features,
            edge_index=batch.edge_index,
            edge_attr=batch.edge_attr,
            pos=batch.pos,
            residue_index=batch.residue_index,
            is_ligand=batch.is_ligand,            # ğŸ‘ˆ æ–°å¢ï¼šå—ä½“é…ä½“æ ‡ç­¾
            mask_interface=batch.mask_interface,  
            batch_idx=batch.batch                 
        )
        
        if rank == 0 and epoch == 0 and batch_idx == 0:
            print("\n[DEBUG] forward finite check")
            print("  z1 finite:", torch.isfinite(z1).all().item())
            print("  z2 finite:", torch.isfinite(z2).all().item())
            print("  pos_pred_v1 finite:", torch.isfinite(pos_pred_v1).all().item())
            print(f"  mask_v1 sum (ç ´åçš„åŸå­æ•°): {mask_v1.sum().item()}")
        
        # ================= ğŸš¨ æ–°çš„æŸå¤±è®¡ç®—æ¥å£ =================
        loss, contrast_loss, recon_loss = criterion(
            z1=z1,
            z2=z2,
            pos_pred_v1=pos_pred_v1,
            pos_true=batch.pos,
            mask_v1=mask_v1,
            batch_idx=batch.batch
        )

        # ğŸš¨ æ ¸å¿ƒé€»è¾‘ï¼šåŠ å…¥ç†µæ­£åˆ™åŒ–æƒ©ç½š (å‡å»ç†µï¼Œå³é¼“åŠ±æ³¨æ„åŠ›åˆ†æ•£)
        # è¿™é‡Œçš„ 0.01 æ˜¯æ§åˆ¶ç†µæ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°
        # ä½¿ç”¨ä¸€ä¸ªæ–°çš„å˜é‡å step_lossï¼Œåƒä¸‡ä¸è¦è¦†ç›–å¤–å±‚çš„ total_loss

        ent_weight = config['training'].get('entropy_weight', 0.01)
        step_loss = loss - ent_weight * batch_entropy
        
        if rank == 0 and epoch == 0 and batch_idx == 0:
            print("\n[DEBUG] loss finite check")
            print("  loss finite:", torch.isfinite(loss).item())
            print("  contrast_loss finite:", torch.isfinite(contrast_loss).item())
            print("  recon_loss finite:", torch.isfinite(recon_loss).item())
        
        optimizer.zero_grad()
        step_loss.backward()  # ğŸ‘ˆ å¯¹ step_loss åå‘ä¼ æ’­
        
        # ================= ğŸš¨ æ–°å¢ï¼šé«˜ç²¾åº¦æ¢¯åº¦æ¢é’ˆ =================
        if rank == 0 and batch_idx % 50 == 0:
            proj_grad = model.module.projector.mlp[0].weight.grad
            attn_grad = model.module.attn_pooling.attn_mlp[0].weight.grad
            scale_grad = criterion.logit_scale.grad
            
            # æ˜¾å¼åˆ¤æ–­ Noneï¼Œå¹¶ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³• (.3e) æ‰“å°æå…¶å¾®å°çš„æ¢¯åº¦
            proj_str = "None" if proj_grad is None else f"{proj_grad.norm().item():.3e}"
            attn_str = "None" if attn_grad is None else f"{attn_grad.norm().item():.3e}"
            scale_str = "None" if scale_grad is None else f"{scale_grad.item():.3e}"
            
            print(f"\n[GRAD CHECK] Proj: {proj_str} | Attn: {attn_str} | Temp Scale: {scale_str}")
        # =========================================================================

        max_grad_norm = config['training']['max_grad_norm']
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        optimizer.step()
        
        # ç´¯åŠ å™¨ä¿æŒ float ç›¸åŠ 
        total_loss += step_loss.item() 
        total_recon_loss += recon_loss.item()
        total_contrast_loss += contrast_loss.item()
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                'loss': f'{step_loss.item():.4f}',
                'recon': f'{recon_loss.item():.4f}',
                'contrast': f'{contrast_loss.item():.4f}'
            })
        
        if (batch_idx + 1) % log_interval == 0 and rank == 0 and wandb_logger is not None:

            current_temp = 1.0 / criterion.logit_scale.exp().item()

            wandb_logger.log({
                'train/loss': step_loss.item(),
                'train/recon_loss': recon_loss.item(),
                'train/contrast_loss': contrast_loss.item(),
                'train/entropy': batch_entropy.item(),  # ğŸ‘ˆ æ–°å¢ï¼šåœ¨ WandB ç›‘æ§æ³¨æ„åŠ›ç†µï¼
                'train/temperature': current_temp,  # ğŸ‘ˆ æ–°å¢ï¼šç›‘æ§å½“å‰æ¸©åº¦ï¼
                'train/learning_rate': optimizer.param_groups[0]['lr'],
                'epoch': epoch,
                'batch': batch_idx
            })
        # =========================================================
    
    return {
        'loss': total_loss / num_batches,
        'recon_loss': total_recon_loss / num_batches,
        'contrast_loss': total_contrast_loss / num_batches
    }

@torch.no_grad()
def validate(
    model,
    val_loader,
    criterion,
    device,
    epoch,
    config,
    rank,
    wandb_logger=None
):
    """éªŒè¯æ¨¡å‹ï¼Œå¹¶è®¡ç®— Top-1 æ£€ç´¢å‡†ç¡®ç‡ã€‚"""
    model.eval()
    total_loss = 0.0
    total_recon_loss = 0.0
    total_contrast_loss = 0.0
    
    # æ–°å¢ï¼šç”¨äºç»Ÿè®¡ç›¸ä¼¼åº¦å’Œå‡†ç¡®ç‡
    all_top1_acc = []
    all_pos_sim = []
    all_neg_sim = []
    
    num_batches = 0
    
    if rank == 0:
        pbar = tqdm(val_loader, desc="Validation")
    else:
        pbar = val_loader
    
    for batch in pbar:
        batch = batch.to(device)
        
        # å‰å‘ä¼ æ’­
        graph_z1, graph_z2, pos_pred_v1, mask_v1, batch_entropy = model(
            z=batch.x,
            vector_features=batch.vector_features,
            edge_index=batch.edge_index,
            edge_attr=batch.edge_attr,
            pos=batch.pos,
            residue_index=batch.residue_index,
            is_ligand=batch.is_ligand,
            mask_interface=batch.mask_interface,
            batch_idx=batch.batch
        )
        
        # è®¡ç®—åŸºç¡€ Loss
        loss, contrast_loss, recon_loss = criterion(
            z1=graph_z1,
            z2=graph_z2,
            pos_pred_v1=pos_pred_v1,
            pos_true=batch.pos,
            mask_v1=mask_v1,
            batch_idx=batch.batch
        )
        
        # ================= ğŸš¨ æ–°å¢ï¼šæ½œåœ¨ç›¸ä¼¼åº¦ä¸æ£€ç´¢æµ‹è¯•é€»è¾‘ =================
        ent_weight = config['training'].get('entropy_weight', 0.01)
        val_total_loss = loss - ent_weight * batch_entropy
        
        # ================= ğŸš¨ ä¿®å¤ 2ï¼šéªŒè¯é›†å…¨å±€æ£€ç´¢ (All-Gather) =================
        if dist.is_initialized():
            z1_list = [torch.zeros_like(graph_z1) for _ in range(dist.get_world_size())]
            z2_list = [torch.zeros_like(graph_z2) for _ in range(dist.get_world_size())]
            dist.all_gather(z1_list, graph_z1)
            dist.all_gather(z2_list, graph_z2)
            
            # æŠŠå½“å‰ rank çš„æ”¾åœ¨æœ€å‰é¢ï¼Œç¡®ä¿å¯¹è§’çº¿ targets ä¾ç„¶æˆç«‹
            rank_idx = dist.get_rank()
            z1_list[rank_idx] = graph_z1
            z2_list[rank_idx] = graph_z2
            
            graph_z1_global = torch.cat(z1_list, dim=0)
            graph_z2_global = torch.cat(z2_list, dim=0)
        else:
            graph_z1_global = graph_z1
            graph_z2_global = graph_z2

        # 1. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
        # å› ä¸º graph_z å·²åš L2 å½’ä¸€åŒ–ï¼Œç‚¹ç§¯å³ä½™å¼¦ç›¸ä¼¼åº¦
        sim_matrix = torch.matmul(graph_z1_global, graph_z2_global.T) 
        
        # 2. è®¡ç®— Top-1 æ£€ç´¢å‡†ç¡®ç‡ (è¿™ä¸€è¡Œé¢„æµ‹çš„æ˜¯ä¸æ˜¯å®ƒè‡ªå·±)
        preds = sim_matrix.argmax(dim=-1)
        targets = torch.arange(sim_matrix.size(0), device=sim_matrix.device)
        top1_acc = (preds == targets).float().mean()
        
        # 3. ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = torch.diagonal(sim_matrix).mean() # å¯¹è§’çº¿ï¼šæ­£æ ·æœ¬
        
        # è´Ÿæ ·æœ¬ï¼šæ’é™¤å¯¹è§’çº¿åçš„å¹³å‡å€¼
        mask_neg = ~torch.eye(sim_matrix.size(0), dtype=torch.bool, device=device)
        neg_sim = sim_matrix[mask_neg].mean()
        
        # æ±‡æ€»
        all_top1_acc.append(top1_acc.item())
        all_pos_sim.append(pos_sim.item())
        all_neg_sim.append(neg_sim.item())
        # ===================================================================
        
        # ğŸš¨ ä¿®å¤ 1 çš„æ”¶å°¾ï¼šç´¯åŠ çš„ loss æ”¹ä¸º val_total_loss
        total_loss += val_total_loss.item() 
        total_recon_loss += recon_loss.item()
        total_contrast_loss += contrast_loss.item()
        num_batches += 1
    
    # è®¡ç®—å…¨éªŒè¯é›†çš„å¹³å‡æŒ‡æ ‡
    avg_loss = total_loss / num_batches
    avg_recon = total_recon_loss / num_batches
    avg_contrast = total_contrast_loss / num_batches
    avg_top1 = sum(all_top1_acc) / len(all_top1_acc)
    avg_pos_sim = sum(all_pos_sim) / len(all_pos_sim)
    avg_neg_sim = sum(all_neg_sim) / len(all_neg_sim)
    
    if rank == 0:
        print(f"\n[VAL REPORT] Top-1 Acc: {avg_top1:.4f} | Pos Sim: {avg_pos_sim:.4f} | Neg Sim: {avg_neg_sim:.4f}")
        
        if wandb_logger is not None:
            wandb_logger.log({
                'val/loss': avg_loss,
                'val/recon_loss': avg_recon,
                'val/contrast_loss': avg_contrast,
                'val/retrieval_top1': avg_top1,    # ğŸ‘ˆ æ£€ç´¢å‡†ç¡®ç‡
                'val/sim_positive': avg_pos_sim,  # ğŸ‘ˆ æ­£æ ·æœ¬ç›¸ä¼¼åº¦
                'val/sim_negative': avg_neg_sim,  # ğŸ‘ˆ è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
                'epoch': epoch
            })
    
    return {
        'loss': avg_loss,
        'recon_loss': avg_recon,
        'contrast_loss': avg_contrast,
        'top1_acc': avg_top1
    }


import os
import torch
import datetime
from torch.nn.parallel import DistributedDataParallel as DDP

def save_checkpoint(
    model,
    optimizer,
    criterion,
    epoch,
    step,
    save_dir,
    is_best=False
):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆä»… rank 0ï¼‰ã€‚"""
    os.makedirs(save_dir, exist_ok=True)
    
    if isinstance(model, DDP):
        model_state_dict = model.module.state_dict()
    else:
        model_state_dict = model.state_dict()
    
    checkpoint = {
        'epoch': epoch,
        'step': step,
        'model_state_dict': model_state_dict,
        'criterion_state_dict': criterion.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    
    # ğŸš¨ è·å–å½“å‰è¿›ç¨‹å· (PID)
    pid = os.getpid()
    
    # ğŸš¨ æ„é€ å¸¦æœ‰ è¿›ç¨‹å· å’Œ epoch çš„æ–‡ä»¶å
    # æ ¼å¼ä¸ºï¼šcheckpoint_è¿›ç¨‹å·_epoch_n.pt
    filename = f"checkpoint_{pid}_epoch_{epoch}.pt"
    save_path = os.path.join(save_dir, filename)
    
    # ä¿å­˜å®ä½“æ–‡ä»¶
    torch.save(checkpoint, save_path)
    
    # é¡ºæ‰‹ä¿å­˜ä¸€ä¸ª `checkpoint_latest.pt` (åŠ ä¸Šè¿›ç¨‹å·ä»¥å…å¤šå¼€ä»»åŠ¡æ—¶äº’ç›¸è¦†ç›–)
    latest_path = os.path.join(save_dir, f'checkpoint_{pid}_latest.pt')
    torch.save(checkpoint, latest_path)
    
    if is_best:
        best_path = os.path.join(save_dir, f'checkpoint_{pid}_best.pt')
        torch.save(checkpoint, best_path)


def main():
    parser = argparse.ArgumentParser(description='GlueVAE CMAE DDP Training')
    parser.add_argument('--config', type=str, default='config_cmae.yaml', help='Path to config file')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint to resume from')
    parser.add_argument('--overfit_test', action='store_true', help='Enable overfit test mode')
    args = parser.parse_args()
    
    # DDP åˆå§‹åŒ–
    dist.init_process_group(
        backend='nccl',
        timeout=timedelta(hours=10)
    )
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ['LOCAL_RANK'])
    
    torch.cuda.set_device(local_rank)
    device = torch.device(f'cuda:{local_rank}')
    
    if rank == 0:
        print(f"=== DDP Training Starting (CMAE) ===")
        print(f"World size: {world_size}")
        print(f"Using device: {device}")
    
    set_seed(args.seed + rank)
    
    config = load_config(args.config)
    
    wandb_logger = setup_wandb(config, rank)
    
    aug_config = config.get('augmentation', {})
    train_aug = aug_config.get('train', {})
    use_rotation = train_aug.get('random_rotation', True)
    
    if rank == 0:
        print("Loading dataset...")
    
    if args.overfit_test:
        if rank == 0:
            print("!!! RUNNING IN OVERFIT TEST MODE !!!")
        batch_size = config['data']['batch_size']
        full_dataset = GlueVAEDataset(
            root=config['data']['root_dir'],
            lmdb_path=config['data']['lmdb_path'],
            split='train',
            exclude_pdb_json=None,
            random_rotation=use_rotation,
            max_samples=batch_size
        )
        train_dataset = full_dataset
        val_dataset = full_dataset
        if rank == 0:
            print(f"Overfit test: Using {len(train_dataset)} samples for both train and val")
            
    else: 
        val_aug = aug_config.get('val', {})
        val_use_rotation = val_aug.get('random_rotation', False)
        
        current_max_samples = config['data'].get('max_samples', None)

        if rank == 0:
            print("Rank 0: å¼€å§‹æ„å»º PyG å…ƒæ–‡ä»¶ä¸ LMDB ç¼“å­˜...")
            train_full_dataset = GlueVAEDataset(
                root=config['data']['root_dir'],
                lmdb_path=config['data']['lmdb_path'],
                split='train',
                exclude_pdb_json=config['data'].get('exclude_pdb_json'),
                random_rotation=use_rotation,
                max_samples=current_max_samples
            )
            val_full_dataset = GlueVAEDataset(
                root=config['data']['root_dir'],
                lmdb_path=config['data']['lmdb_path'],
                split='val',
                exclude_pdb_json=config['data'].get('exclude_pdb_json'),
                random_rotation=val_use_rotation,
                max_samples=current_max_samples
            )
            total_len = len(train_full_dataset)
            _ = len(val_full_dataset)
            print("Rank 0: ç¼“å­˜æ„å»ºå®Œæ¯•ï¼")

        dist.barrier(device_ids=[local_rank])

        if rank != 0:
            import time
            print(f"Rank {rank}: æ­£åœ¨ç­‰å¾… NFS åŒæ­¥ (10ç§’)...")
            time.sleep(10) 
            
            train_full_dataset = GlueVAEDataset(
                root=config['data']['root_dir'],
                lmdb_path=config['data']['lmdb_path'],
                split='train',
                exclude_pdb_json=config['data'].get('exclude_pdb_json'),
                random_rotation=use_rotation,
                max_samples=current_max_samples
            )
            val_full_dataset = GlueVAEDataset(
                root=config['data']['root_dir'],
                lmdb_path=config['data']['lmdb_path'],
                split='val',
                exclude_pdb_json=config['data'].get('exclude_pdb_json'),
                random_rotation=val_use_rotation,
                max_samples=current_max_samples
            )
            total_len = len(train_full_dataset)

        dist.barrier(device_ids=[local_rank])

        # ğŸ‘‡ -------- æŠŠè¿™å‡ è¡Œç²˜è´´è¿›å» -------- ğŸ‘‡
        # ç›´æ¥ä½¿ç”¨æˆ‘ä»¬åœ¨ä¸Šé¢é…ç½®å¥½çš„ä¸“é—¨çš„ train å’Œ val æ•°æ®é›†ï¼Œæœç»ç´¢å¼•é”™ä½ï¼
        train_dataset = train_full_dataset
        val_dataset = val_full_dataset
        # ğŸ‘† -------- ç²˜è´´ç»“æŸ -------- ğŸ‘†

        if rank == 0:
            print(f"Total dataset size: {total_len}")
            print(f"Train dataset size: {len(train_dataset)}")
            print(f"Val dataset size: {len(val_dataset)}")

    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    
    val_sampler = DistributedSampler(
        val_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )
    
    train_loader = PyGDataLoader(
        train_dataset,
        batch_size=config['data']['batch_size'],
        sampler=train_sampler,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory'],
        persistent_workers=(config['data']['num_workers'] > 0)
    )
    
    val_loader = PyGDataLoader(
        val_dataset,
        batch_size=config['data']['batch_size'],
        sampler=val_sampler,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory'],
        persistent_workers=(config['data']['num_workers'] > 0)
    )
    
    if rank == 0:
        print("Initializing model...")
    
    # ğŸš¨ æ³¨æ„ï¼šè¿™é‡Œä¸è¦ä¼  latent_dim äº†ï¼ŒProjector é‡Œå†™æ­»æˆ–è€…åœ¨ config é‡ŒåŠ  proj_dim
    model = GlueVAE(
        hidden_dim=config['model']['hidden_dim'],
        num_encoder_layers=config['model']['num_encoder_layers'],
        num_decoder_layers=config['model']['num_decoder_layers'],
        edge_dim=config['model']['edge_dim'],
        vocab_size=config['model']['vocab_size'],
        use_gradient_checkpointing=config['model']['use_gradient_checkpointing'],
        mask_noise=config['model'].get('mask_noise',0.5)
    ).to(device)
    
    model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)

    if rank == 0:
        num_params = sum(p.numel() for p in model.parameters())
        print(f"Model parameters: {num_params:,}")
    
    # ================= ğŸš¨ æ–°å¢ï¼šå®ä¾‹åŒ– CMAELoss =================
    # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–è¶…å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨é»˜è®¤å€¼
    temp = config['training'].get('temperature', 0.1)
    l_contrast = config['training'].get('lambda_contrast', 1.0)
    l_recon = config['training'].get('lambda_recon', 0.5)
    recon_cutoff = config['training'].get('recon_cutoff', 15.0) # ğŸ‘ˆ æ–°å¢è¯»å–
    
    criterion = CMAELoss(
        temperature=temp,
        lambda_contrast=l_contrast,
        lambda_recon=l_recon,
        cutoff=recon_cutoff  # å»ºè®®ä½¿ç”¨æˆªæ–­ä»¥é™åˆ¶æŸå¤±ä»…åœ¨å±€éƒ¨
    ).to(device)
    # =========================================================
    
    optimizer_config = config['training']['optimizer']
    if optimizer_config['type'] == 'Adam':
        optimizer = optim.Adam(
        [
        {'params': model.parameters()},  # ç¬¬ä¸€ç»„ï¼šæ¨¡å‹çš„å‚æ•°
        {'params': criterion.parameters()} # ç¬¬äºŒç»„ï¼šæŸå¤±å‡½æ•°çš„å‚æ•°
        ],
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay'],
            betas=tuple(optimizer_config['betas']),
            eps=optimizer_config['eps']
        )
    else:
        raise ValueError(f"Unknown optimizer type: {optimizer_config['type']}")
    
    scheduler_config = config['training']['scheduler']
    if scheduler_config['type'] == 'ReduceLROnPlateau':
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=scheduler_config['factor'],
            patience=scheduler_config['patience'],
            min_lr=scheduler_config['min_lr']
        )
    else:
        scheduler = None
    
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')
    
    if args.resume is not None:
        if rank == 0:
            print(f"Resuming from checkpoint: {args.resume}")
        
        checkpoint = torch.load(args.resume, map_location=device)
        
        if isinstance(model, DDP):
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
            
        # ğŸš¨ æ–°å¢ï¼šå®‰å…¨åŠ è½½ criterion çš„çŠ¶æ€ï¼ˆä¸ºäº†å…¼å®¹æ—§ç‰ˆæ²¡æœ‰ä¿å­˜è¯¥å­—æ®µçš„ checkpointï¼‰
        if 'criterion_state_dict' in checkpoint:
            criterion.load_state_dict(checkpoint['criterion_state_dict'])

        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        global_step = checkpoint['step']
        
        if rank == 0:
            print(f"âœ… Resumed at epoch {start_epoch}, step {global_step}")
    
    if rank == 0:
        print("Starting training...")
    
    save_dir = config['logging']['save_dir']
    
    for epoch in range(start_epoch, config['training']['num_epochs']):
        train_sampler.set_epoch(epoch)
        
        # ğŸš¨ æ³¨æ„å»æ‰äº† beta_scheduler ä¼ å‚
        train_metrics = train_epoch(
            model, train_loader, optimizer, criterion,
            device, epoch, config, rank, wandb_logger
        )
        
        if rank == 0:
            # æ‰“å°æ—¥å¿—æ¢æˆäº† contrast_loss
            print(f"Epoch {epoch} - Train: loss={train_metrics['loss']:.4f}, "
                  f"recon={train_metrics['recon_loss']:.4f}, "
                  f"contrast={train_metrics['contrast_loss']:.4f}")
        
        val_interval_epochs = config['logging'].get('val_interval', 5)
        should_validate = (epoch + 1) % val_interval_epochs == 0 or (epoch == config['training']['num_epochs'] - 1)
        
        if should_validate:
            val_sampler.set_epoch(epoch)
            val_metrics = validate(
                model, val_loader, criterion, device, epoch, config, rank, wandb_logger
            )
            
            if rank == 0:
                print(f"Epoch {epoch} - Val: loss={val_metrics['loss']:.4f}, "
                      f"recon={val_metrics['recon_loss']:.4f}, "
                      f"contrast={val_metrics['contrast_loss']:.4f}")
                
                if scheduler is not None:
                    scheduler.step(val_metrics['loss'])
                
                is_best = val_metrics['loss'] < best_val_loss
                if is_best:
                    best_val_loss = val_metrics['loss']
                    print(f"New best val loss: {best_val_loss:.4f}")
                
                global_step = (epoch + 1) * len(train_loader)
                save_checkpoint(model, optimizer, criterion, epoch, global_step, save_dir, is_best)
        else:
            save_interval_epochs = config['logging'].get('save_interval', 10)
            if rank == 0 and (epoch + 1) % save_interval_epochs == 0:
                global_step = (epoch + 1) * len(train_loader)
                save_checkpoint(model, optimizer, criterion, epoch, global_step, save_dir, is_best=False)
    
    if rank == 0:
        print("Training complete!")
        if wandb_logger is not None:
            wandb_logger.finish()
    
    dist.destroy_process_group()


if __name__ == "__main__":
    main()